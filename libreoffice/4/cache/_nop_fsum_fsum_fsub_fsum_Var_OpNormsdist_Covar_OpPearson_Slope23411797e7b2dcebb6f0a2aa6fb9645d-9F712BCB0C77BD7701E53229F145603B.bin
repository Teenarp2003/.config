//
// Generated by NVIDIA NVVM Compiler
//
// Compiler Build ID: CL-32603126
// Unknown Toolkit Version
// Based on NVVM 7.0.1
//

.version 8.1
.target sm_75, texmode_independent
.address_size 64

	// .globl	DynamicKernel_nop_fsum_fsum_fsub_fsum_Var_OpNormsdist_Covar_OpPearson_Slope

.entry DynamicKernel_nop_fsum_fsum_fsub_fsum_Var_OpNormsdist_Covar_OpPearson_Slope(
	.param .u64 .ptr .global .align 8 DynamicKernel_nop_fsum_fsum_fsub_fsum_Var_OpNormsdist_Covar_OpPearson_Slope_param_0,
	.param .u64 .ptr .global .align 8 DynamicKernel_nop_fsum_fsum_fsub_fsum_Var_OpNormsdist_Covar_OpPearson_Slope_param_1,
	.param .u64 .ptr .global .align 8 DynamicKernel_nop_fsum_fsum_fsub_fsum_Var_OpNormsdist_Covar_OpPearson_Slope_param_2,
	.param .u64 .ptr .global .align 8 DynamicKernel_nop_fsum_fsum_fsub_fsum_Var_OpNormsdist_Covar_OpPearson_Slope_param_3,
	.param .u64 .ptr .global .align 8 DynamicKernel_nop_fsum_fsum_fsub_fsum_Var_OpNormsdist_Covar_OpPearson_Slope_param_4,
	.param .u64 .ptr .global .align 8 DynamicKernel_nop_fsum_fsum_fsub_fsum_Var_OpNormsdist_Covar_OpPearson_Slope_param_5,
	.param .u64 .ptr .global .align 8 DynamicKernel_nop_fsum_fsum_fsub_fsum_Var_OpNormsdist_Covar_OpPearson_Slope_param_6,
	.param .u64 .ptr .global .align 8 DynamicKernel_nop_fsum_fsum_fsub_fsum_Var_OpNormsdist_Covar_OpPearson_Slope_param_7,
	.param .u64 .ptr .global .align 8 DynamicKernel_nop_fsum_fsum_fsub_fsum_Var_OpNormsdist_Covar_OpPearson_Slope_param_8,
	.param .u64 .ptr .global .align 8 DynamicKernel_nop_fsum_fsum_fsub_fsum_Var_OpNormsdist_Covar_OpPearson_Slope_param_9
)
{
	.reg .pred 	%p<358>;
	.reg .f32 	%f<13>;
	.reg .b32 	%r<306>;
	.reg .f64 	%fd<1563>;
	.reg .b64 	%rd<71>;


	ld.param.u64 	%rd17, [DynamicKernel_nop_fsum_fsum_fsub_fsum_Var_OpNormsdist_Covar_OpPearson_Slope_param_1];
	ld.param.u64 	%rd18, [DynamicKernel_nop_fsum_fsum_fsub_fsum_Var_OpNormsdist_Covar_OpPearson_Slope_param_2];
	ld.param.u64 	%rd19, [DynamicKernel_nop_fsum_fsum_fsub_fsum_Var_OpNormsdist_Covar_OpPearson_Slope_param_3];
	ld.param.u64 	%rd20, [DynamicKernel_nop_fsum_fsum_fsub_fsum_Var_OpNormsdist_Covar_OpPearson_Slope_param_4];
	ld.param.u64 	%rd21, [DynamicKernel_nop_fsum_fsum_fsub_fsum_Var_OpNormsdist_Covar_OpPearson_Slope_param_5];
	ld.param.u64 	%rd22, [DynamicKernel_nop_fsum_fsum_fsub_fsum_Var_OpNormsdist_Covar_OpPearson_Slope_param_6];
	ld.param.u64 	%rd23, [DynamicKernel_nop_fsum_fsum_fsub_fsum_Var_OpNormsdist_Covar_OpPearson_Slope_param_7];
	ld.param.u64 	%rd24, [DynamicKernel_nop_fsum_fsum_fsub_fsum_Var_OpNormsdist_Covar_OpPearson_Slope_param_8];
	ld.param.u64 	%rd25, [DynamicKernel_nop_fsum_fsum_fsub_fsum_Var_OpNormsdist_Covar_OpPearson_Slope_param_9];
	mov.b32 	%r81, %envreg3;
	mov.u32 	%r82, %ctaid.x;
	mov.u32 	%r83, %ntid.x;
	mov.u32 	%r84, %tid.x;
	add.s32 	%r85, %r84, %r81;
	mad.lo.s32 	%r1, %r83, %r82, %r85;
	cvt.s64.s32 	%rd1, %r1;
	setp.gt.s32 	%p1, %r1, 5;
	mov.f64 	%fd1483, 0d0000000000000000;
	mov.f64 	%fd1484, %fd1483;
	mov.f64 	%fd1485, %fd1483;
	@%p1 bra 	$L__BB0_6;

	mov.u32 	%r279, 1;
	mov.u32 	%r278, %r1;

$L__BB0_2:
	mul.wide.s32 	%rd26, %r278, 8;
	add.s64 	%rd27, %rd24, %rd26;
	add.s64 	%rd28, %rd25, %rd26;
	ld.global.f64 	%fd4, [%rd28];
	ld.global.f64 	%fd5, [%rd27];
	abs.f64 	%fd253, %fd5;
	setp.gtu.f64 	%p2, %fd253, 0d7FF0000000000000;
	@%p2 bra 	$L__BB0_5;

	abs.f64 	%fd254, %fd4;
	setp.gtu.f64 	%p3, %fd254, 0d7FF0000000000000;
	@%p3 bra 	$L__BB0_5;

	add.f64 	%fd1485, %fd1485, %fd4;
	add.f64 	%fd1484, %fd1484, %fd5;
	add.f64 	%fd1483, %fd1483, 0d3FF0000000000000;

$L__BB0_5:
	add.s32 	%r278, %r279, %r1;
	setp.lt.s32 	%p4, %r278, 6;
	setp.lt.u32 	%p5, %r279, 2;
	mov.u32 	%r279, 2;
	and.pred  	%p6, %p5, %p4;
	@%p6 bra 	$L__BB0_2;

$L__BB0_6:
	setp.lt.f64 	%p7, %fd1483, 0d3FF0000000000000;
	mov.f64 	%fd1495, 0d7FF8000000000207;
	@%p7 bra 	$L__BB0_15;

	div.rn.f64 	%fd15, %fd1485, %fd1483;
	div.rn.f64 	%fd16, %fd1484, %fd1483;
	mov.f64 	%fd1491, 0d0000000000000000;
	mov.f64 	%fd1492, %fd1491;
	@%p1 bra 	$L__BB0_13;

	mov.u32 	%r281, 1;
	mov.u32 	%r280, %r1;

$L__BB0_9:
	mul.wide.s32 	%rd29, %r280, 8;
	add.s64 	%rd30, %rd24, %rd29;
	add.s64 	%rd31, %rd25, %rd29;
	ld.global.f64 	%fd19, [%rd31];
	ld.global.f64 	%fd20, [%rd30];
	abs.f64 	%fd260, %fd20;
	setp.gtu.f64 	%p9, %fd260, 0d7FF0000000000000;
	@%p9 bra 	$L__BB0_12;

	abs.f64 	%fd261, %fd19;
	setp.gtu.f64 	%p10, %fd261, 0d7FF0000000000000;
	@%p10 bra 	$L__BB0_12;

	sub.f64 	%fd262, %fd19, %fd15;
	sub.f64 	%fd263, %fd20, %fd16;
	fma.rn.f64 	%fd1492, %fd262, %fd263, %fd1492;
	fma.rn.f64 	%fd1491, %fd262, %fd262, %fd1491;

$L__BB0_12:
	add.s32 	%r280, %r281, %r1;
	setp.lt.s32 	%p11, %r280, 6;
	setp.lt.u32 	%p12, %r281, 2;
	mov.u32 	%r281, 2;
	and.pred  	%p13, %p12, %p11;
	@%p13 bra 	$L__BB0_9;

$L__BB0_13:
	setp.eq.f64 	%p14, %fd1491, 0d0000000000000000;
	mov.f64 	%fd1495, 0d7FF8000000000214;
	@%p14 bra 	$L__BB0_15;

	div.rn.f64 	%fd1495, %fd1492, %fd1491;

$L__BB0_15:
	add.f64 	%fd29, %fd1495, 0d0000000000000000;
	mov.f64 	%fd1499, 0d0000000000000000;
	mov.f64 	%fd1500, %fd1499;
	mov.f64 	%fd1501, %fd1499;
	@%p1 bra 	$L__BB0_21;

	mov.u32 	%r283, 1;
	mov.u32 	%r282, %r1;

$L__BB0_17:
	mul.wide.s32 	%rd32, %r282, 8;
	add.s64 	%rd33, %rd22, %rd32;
	add.s64 	%rd34, %rd23, %rd32;
	ld.global.f64 	%fd33, [%rd34];
	ld.global.f64 	%fd34, [%rd33];
	abs.f64 	%fd271, %fd34;
	setp.gtu.f64 	%p16, %fd271, 0d7FF0000000000000;
	@%p16 bra 	$L__BB0_20;

	abs.f64 	%fd272, %fd33;
	setp.gtu.f64 	%p17, %fd272, 0d7FF0000000000000;
	@%p17 bra 	$L__BB0_20;

	add.f64 	%fd1501, %fd1501, %fd33;
	add.f64 	%fd1500, %fd1500, %fd34;
	add.f64 	%fd1499, %fd1499, 0d3FF0000000000000;

$L__BB0_20:
	add.s32 	%r282, %r283, %r1;
	setp.lt.s32 	%p18, %r282, 6;
	setp.lt.u32 	%p19, %r283, 2;
	mov.u32 	%r283, 2;
	and.pred  	%p20, %p19, %p18;
	@%p20 bra 	$L__BB0_17;

$L__BB0_21:
	setp.lt.f64 	%p21, %fd1499, 0d3FF0000000000000;
	mov.f64 	%fd1514, 0d7FF8000000000207;
	@%p21 bra 	$L__BB0_30;

	div.rn.f64 	%fd44, %fd1501, %fd1499;
	div.rn.f64 	%fd45, %fd1500, %fd1499;
	mov.f64 	%fd1508, 0d0000000000000000;
	mov.f64 	%fd1509, %fd1508;
	mov.f64 	%fd1510, %fd1508;
	@%p1 bra 	$L__BB0_28;

	mov.u32 	%r285, 1;
	mov.u32 	%r284, %r1;

$L__BB0_24:
	mul.wide.s32 	%rd35, %r284, 8;
	add.s64 	%rd36, %rd22, %rd35;
	add.s64 	%rd37, %rd23, %rd35;
	ld.global.f64 	%fd49, [%rd37];
	ld.global.f64 	%fd50, [%rd36];
	abs.f64 	%fd280, %fd50;
	setp.gtu.f64 	%p23, %fd280, 0d7FF0000000000000;
	@%p23 bra 	$L__BB0_27;

	abs.f64 	%fd281, %fd49;
	setp.gtu.f64 	%p24, %fd281, 0d7FF0000000000000;
	@%p24 bra 	$L__BB0_27;

	sub.f64 	%fd282, %fd49, %fd44;
	sub.f64 	%fd283, %fd50, %fd45;
	fma.rn.f64 	%fd1510, %fd282, %fd283, %fd1510;
	fma.rn.f64 	%fd1509, %fd282, %fd282, %fd1509;
	fma.rn.f64 	%fd1508, %fd283, %fd283, %fd1508;

$L__BB0_27:
	add.s32 	%r284, %r285, %r1;
	setp.lt.s32 	%p25, %r284, 6;
	setp.lt.u32 	%p26, %r285, 2;
	mov.u32 	%r285, 2;
	and.pred  	%p27, %p26, %p25;
	@%p27 bra 	$L__BB0_24;

$L__BB0_28:
	setp.eq.f64 	%p28, %fd1508, 0d0000000000000000;
	setp.eq.f64 	%p29, %fd1509, 0d0000000000000000;
	or.pred  	%p30, %p28, %p29;
	mov.f64 	%fd1514, 0d7FF8000000000214;
	@%p30 bra 	$L__BB0_30;

	mul.f64 	%fd285, %fd1508, %fd1509;
	sqrt.rn.f64 	%fd286, %fd285;
	div.rn.f64 	%fd1514, %fd1510, %fd286;

$L__BB0_30:
	add.f64 	%fd62, %fd1514, 0d0000000000000000;
	mov.f64 	%fd1518, 0d0000000000000000;
	mov.f64 	%fd1519, %fd1518;
	mov.f64 	%fd1520, %fd1518;
	@%p1 bra 	$L__BB0_36;

	mov.u32 	%r287, 1;
	mov.u32 	%r286, %r1;

$L__BB0_32:
	mul.wide.s32 	%rd38, %r286, 8;
	add.s64 	%rd39, %rd20, %rd38;
	add.s64 	%rd40, %rd21, %rd38;
	ld.global.f64 	%fd66, [%rd40];
	ld.global.f64 	%fd67, [%rd39];
	abs.f64 	%fd293, %fd67;
	setp.gtu.f64 	%p32, %fd293, 0d7FF0000000000000;
	@%p32 bra 	$L__BB0_35;

	abs.f64 	%fd294, %fd66;
	setp.gtu.f64 	%p33, %fd294, 0d7FF0000000000000;
	@%p33 bra 	$L__BB0_35;

	add.f64 	%fd1520, %fd1520, %fd67;
	add.f64 	%fd1519, %fd1519, %fd66;
	add.f64 	%fd1518, %fd1518, 0d3FF0000000000000;

$L__BB0_35:
	add.s32 	%r286, %r287, %r1;
	setp.lt.s32 	%p34, %r286, 6;
	setp.lt.u32 	%p35, %r287, 2;
	mov.u32 	%r287, 2;
	and.pred  	%p36, %p35, %p34;
	@%p36 bra 	$L__BB0_32;

$L__BB0_36:
	setp.lt.f64 	%p37, %fd1518, 0d3FF0000000000000;
	mov.f64 	%fd1527, 0d7FF8000000000207;
	@%p37 bra 	$L__BB0_44;

	mov.f64 	%fd1525, 0d0000000000000000;
	@%p1 bra 	$L__BB0_43;

	div.rn.f64 	%fd77, %fd1519, %fd1518;
	div.rn.f64 	%fd78, %fd1520, %fd1518;
	mov.u32 	%r289, 1;
	mov.u32 	%r288, %r1;

$L__BB0_39:
	mul.wide.s32 	%rd41, %r288, 8;
	add.s64 	%rd42, %rd20, %rd41;
	add.s64 	%rd43, %rd21, %rd41;
	ld.global.f64 	%fd80, [%rd43];
	ld.global.f64 	%fd81, [%rd42];
	abs.f64 	%fd298, %fd81;
	setp.gtu.f64 	%p39, %fd298, 0d7FF0000000000000;
	@%p39 bra 	$L__BB0_42;

	abs.f64 	%fd299, %fd80;
	setp.gtu.f64 	%p40, %fd299, 0d7FF0000000000000;
	@%p40 bra 	$L__BB0_42;

	sub.f64 	%fd300, %fd81, %fd78;
	sub.f64 	%fd301, %fd80, %fd77;
	fma.rn.f64 	%fd1525, %fd300, %fd301, %fd1525;

$L__BB0_42:
	add.s32 	%r288, %r289, %r1;
	setp.lt.s32 	%p41, %r288, 6;
	setp.lt.u32 	%p42, %r289, 2;
	mov.u32 	%r289, 2;
	and.pred  	%p43, %p42, %p41;
	@%p43 bra 	$L__BB0_39;

$L__BB0_43:
	div.rn.f64 	%fd1527, %fd1525, %fd1518;

$L__BB0_44:
	setp.gt.s32 	%p44, %r1, 4;
	mov.f64 	%fd1528, 0d7FFFFFFFE0000000;
	@%p44 bra 	$L__BB0_46;

	shl.b64 	%rd44, %rd1, 3;
	add.s64 	%rd45, %rd19, %rd44;
	ld.global.f64 	%fd1528, [%rd45];

$L__BB0_46:
	abs.f64 	%fd304, %fd1528;
	setp.le.f64 	%p45, %fd304, 0d7FF0000000000000;
	mul.f64 	%fd305, %fd1528, 0dBFE6A09E667F3BCC;
	selp.f64 	%fd303, %fd305, 0d8000000000000000, %p45;
	// begin inline asm
	{ 
	.reg 	.b32 lo; 
	mov.b64 	{lo, %r98}, %fd303; 
	}
	// end inline asm
	setp.lt.s32 	%p46, %r98, 1072168960;
	@%p46 bra 	$L__BB0_51;
	bra.uni 	$L__BB0_47;

$L__BB0_51:
	abs.f64 	%fd552, %fd303;
	mov.f64 	%fd553, 0d3D47088FDB46FA5F;
	mov.f64 	%fd554, 0dBCF0679AFBA6F279;
	fma.rn.f64 	%fd555, %fd554, %fd552, %fd553;
	mov.f64 	%fd556, 0dBD8DF9F9B976A9B2;
	fma.rn.f64 	%fd557, %fd555, %fd552, %fd556;
	mov.f64 	%fd558, 0d3DC7F1F5590CC332;
	fma.rn.f64 	%fd559, %fd557, %fd552, %fd558;
	mov.f64 	%fd560, 0dBDFA28A3CD2D56C4;
	fma.rn.f64 	%fd561, %fd559, %fd552, %fd560;
	mov.f64 	%fd562, 0d3E2485EE67835925;
	fma.rn.f64 	%fd563, %fd561, %fd552, %fd562;
	mov.f64 	%fd564, 0dBE476DB45919F583;
	fma.rn.f64 	%fd565, %fd563, %fd552, %fd564;
	mov.f64 	%fd566, 0d3E62D698D98C8D71;
	fma.rn.f64 	%fd567, %fd565, %fd552, %fd566;
	mov.f64 	%fd568, 0dBE720A2C7155D5C6;
	fma.rn.f64 	%fd569, %fd567, %fd552, %fd568;
	mov.f64 	%fd570, 0dBE41D29B37CA1397;
	fma.rn.f64 	%fd571, %fd569, %fd552, %fd570;
	mov.f64 	%fd572, 0d3EA2EF6CC0F67A49;
	fma.rn.f64 	%fd573, %fd571, %fd552, %fd572;
	mov.f64 	%fd574, 0dBEC102B892333B6F;
	fma.rn.f64 	%fd575, %fd573, %fd552, %fd574;
	mov.f64 	%fd576, 0d3ECA30375BA9A84E;
	fma.rn.f64 	%fd577, %fd575, %fd552, %fd576;
	mov.f64 	%fd578, 0d3ECAAD18DEDEA43E;
	fma.rn.f64 	%fd579, %fd577, %fd552, %fd578;
	mov.f64 	%fd580, 0dBEFF05355BC5B225;
	fma.rn.f64 	%fd581, %fd579, %fd552, %fd580;
	mov.f64 	%fd582, 0d3F10E37A3108BC8B;
	fma.rn.f64 	%fd583, %fd581, %fd552, %fd582;
	mov.f64 	%fd584, 0d3EFB292D828E5CB2;
	fma.rn.f64 	%fd585, %fd583, %fd552, %fd584;
	mov.f64 	%fd586, 0dBF4356626EBF9BFA;
	fma.rn.f64 	%fd587, %fd585, %fd552, %fd586;
	mov.f64 	%fd588, 0d3F5BCA68F73D6AFC;
	fma.rn.f64 	%fd589, %fd587, %fd552, %fd588;
	mov.f64 	%fd590, 0dBF2B6B69EBBC280B;
	fma.rn.f64 	%fd591, %fd589, %fd552, %fd590;
	mov.f64 	%fd592, 0dBF9396685912A453;
	fma.rn.f64 	%fd593, %fd591, %fd552, %fd592;
	mov.f64 	%fd594, 0d3FBA4F4E2A1ABEF8;
	fma.rn.f64 	%fd595, %fd593, %fd552, %fd594;
	mov.f64 	%fd596, 0d3FE45F306DC9C8BB;
	fma.rn.f64 	%fd597, %fd595, %fd552, %fd596;
	mov.f64 	%fd598, 0d3FC06EBA8214DB69;
	fma.rn.f64 	%fd599, %fd597, %fd552, %fd598;
	fma.rn.f64 	%fd600, %fd599, %fd552, %fd552;
	sub.f64 	%fd601, %fd552, %fd600;
	fma.rn.f64 	%fd602, %fd599, %fd552, %fd601;
	neg.f64 	%fd603, %fd600;
	neg.f64 	%fd604, %fd602;
	cvt.rn.f32.f64 	%f5, %fd600;
	mul.f32 	%f6, %f5, 0fBFB8AA3B;
	cvt.rni.f32.f32 	%f7, %f6;
	cvt.f64.f32 	%fd605, %f7;
	neg.f64 	%fd606, %fd605;
	mov.f64 	%fd607, 0d3FE62E42FEFA39EF;
	fma.rn.f64 	%fd608, %fd606, %fd607, %fd603;
	mov.f64 	%fd609, 0d3E928A27F89B6999;
	mov.f64 	%fd610, 0d3E5AE904A4741B81;
	fma.rn.f64 	%fd611, %fd610, %fd608, %fd609;
	mov.f64 	%fd612, 0d3EC71DE715FF7E07;
	fma.rn.f64 	%fd613, %fd611, %fd608, %fd612;
	mov.f64 	%fd614, 0d3EFA019A6B0AC45A;
	fma.rn.f64 	%fd615, %fd613, %fd608, %fd614;
	mov.f64 	%fd616, 0d3F2A01A017EED94F;
	fma.rn.f64 	%fd617, %fd615, %fd608, %fd616;
	mov.f64 	%fd618, 0d3F56C16C17F2A71B;
	fma.rn.f64 	%fd619, %fd617, %fd608, %fd618;
	mov.f64 	%fd620, 0d3F811111111173C4;
	fma.rn.f64 	%fd621, %fd619, %fd608, %fd620;
	mov.f64 	%fd622, 0d3FA555555555211A;
	fma.rn.f64 	%fd623, %fd621, %fd608, %fd622;
	mov.f64 	%fd624, 0d3FC5555555555540;
	fma.rn.f64 	%fd625, %fd623, %fd608, %fd624;
	mov.f64 	%fd626, 0d3FE0000000000005;
	fma.rn.f64 	%fd627, %fd625, %fd608, %fd626;
	mul.f64 	%fd628, %fd608, %fd627;
	fma.rn.f64 	%fd629, %fd628, %fd608, %fd604;
	add.f64 	%fd630, %fd608, %fd629;
	ex2.approx.ftz.f32 	%f8, %f7;
	cvt.f64.f32 	%fd631, %f8;
	mov.f64 	%fd632, 0d3FF0000000000000;
	sub.f64 	%fd633, %fd632, %fd631;
	neg.f64 	%fd634, %fd630;
	fma.rn.f64 	%fd635, %fd634, %fd631, %fd633;
	setp.ge.f64 	%p51, %fd552, 0d4017AFB48DC96626;
	selp.f64 	%fd636, 0d3FF0000000000000, %fd635, %p51;
	{
	.reg .b32 %temp; 
	mov.b64 	{%r121, %temp}, %fd636;
	}
	{
	.reg .b32 %temp; 
	mov.b64 	{%temp, %r122}, %fd636;
	}
	{
	.reg .b32 %temp; 
	mov.b64 	{%temp, %r123}, %fd303;
	}
	and.b32  	%r124, %r123, -2147483648;
	or.b32  	%r125, %r122, %r124;
	mov.b64 	%fd637, {%r121, %r125};
	sub.f64 	%fd1529, %fd632, %fd637;
	bra.uni 	$L__BB0_52;

$L__BB0_47:
	setp.gt.f64 	%p47, %fd303, 0d403B4CCCCCCCCCCD;
	mov.f64 	%fd1529, 0d0000000000000000;
	@%p47 bra 	$L__BB0_52;

	setp.lt.s32 	%p48, %r98, 1075052544;
	@%p48 bra 	$L__BB0_50;
	bra.uni 	$L__BB0_49;

$L__BB0_50:
	mov.f64 	%fd425, 0d3FE20DD7452FBC22;
	mov.f64 	%fd427, 0d401FD453E105E9A2;
	// begin inline asm
	fma.rn.f64 	%fd424, %fd425, %fd303, %fd427;
	// end inline asm
	mov.f64 	%fd431, 0d404B26245B951FB4;
	// begin inline asm
	fma.rn.f64 	%fd428, %fd424, %fd303, %fd431;
	// end inline asm
	mov.f64 	%fd435, 0d406C7835DC0F1F49;
	// begin inline asm
	fma.rn.f64 	%fd432, %fd428, %fd303, %fd435;
	// end inline asm
	mov.f64 	%fd439, 0d4083AFA471E5C766;
	// begin inline asm
	fma.rn.f64 	%fd436, %fd432, %fd303, %fd439;
	// end inline asm
	mov.f64 	%fd443, 0d4091FB514824F49F;
	// begin inline asm
	fma.rn.f64 	%fd440, %fd436, %fd303, %fd443;
	// end inline asm
	mov.f64 	%fd447, 0d409450DDEE8272BB;
	// begin inline asm
	fma.rn.f64 	%fd444, %fd440, %fd303, %fd447;
	// end inline asm
	mov.f64 	%fd451, 0d4086B952E4ECBC50;
	// begin inline asm
	fma.rn.f64 	%fd448, %fd444, %fd303, %fd451;
	// end inline asm
	add.f64 	%fd453, %fd303, 0d402C35442E99E667;
	mov.f64 	%fd455, 0d40582F68071A079D;
	// begin inline asm
	fma.rn.f64 	%fd452, %fd453, %fd303, %fd455;
	// end inline asm
	mov.f64 	%fd459, 0d4079ABD39A029DAA;
	// begin inline asm
	fma.rn.f64 	%fd456, %fd452, %fd303, %fd459;
	// end inline asm
	mov.f64 	%fd463, 0d409230CA327093FD;
	// begin inline asm
	fma.rn.f64 	%fd460, %fd456, %fd303, %fd463;
	// end inline asm
	mov.f64 	%fd467, 0d40A174FAB33B54A7;
	// begin inline asm
	fma.rn.f64 	%fd464, %fd460, %fd303, %fd467;
	// end inline asm
	mov.f64 	%fd471, 0d40A601508230F980;
	// begin inline asm
	fma.rn.f64 	%fd468, %fd464, %fd303, %fd471;
	// end inline asm
	mov.f64 	%fd475, 0d40A091785EC9331E;
	// begin inline asm
	fma.rn.f64 	%fd472, %fd468, %fd303, %fd475;
	// end inline asm
	mov.f64 	%fd479, 0d4086B952E52F3622;
	// begin inline asm
	fma.rn.f64 	%fd476, %fd472, %fd303, %fd479;
	// end inline asm
	div.rn.f64 	%fd545, %fd448, %fd476;
	mul.rn.f64 	%fd546, %fd303, %fd303;
	neg.f64 	%fd487, %fd546;
	// begin inline asm
	fma.rn.f64 	%fd480, %fd303, %fd303, %fd487;
	// end inline asm
	mov.f64 	%fd547, 0d3FF71547652B82FE;
	mul.rn.f64 	%fd548, %fd487, %fd547;
	{
	.reg .b32 %temp; 
	mov.b64 	{%temp, %r112}, %fd548;
	}
	and.b32  	%r113, %r112, -2147483648;
	mov.f64 	%fd531, 0d3FE0000000000000;
	{
	.reg .b32 %temp; 
	mov.b64 	{%temp, %r114}, %fd531;
	}
	or.b32  	%r115, %r114, %r113;
	{
	.reg .b32 %temp; 
	mov.b64 	{%r116, %temp}, %fd531;
	}
	mov.b64 	%fd549, {%r116, %r115};
	add.rz.f64 	%fd550, %fd548, %fd549;
	cvt.rzi.f64.f64 	%fd489, %fd550;
	cvt.rzi.s32.f64 	%r117, %fd489;
	mov.f64 	%fd486, 0dBFE62E42FEFA39EF;
	// begin inline asm
	fma.rn.f64 	%fd484, %fd489, %fd486, %fd487;
	// end inline asm
	mov.f64 	%fd490, 0dBC7ABC9E3B39803F;
	// begin inline asm
	fma.rn.f64 	%fd488, %fd489, %fd490, %fd484;
	// end inline asm
	setp.lt.s32 	%p50, %r117, -1020;
	selp.f64 	%fd551, 0d3C90000000000000, 0d4000000000000000, %p50;
	mov.f64 	%fd493, 0d3E21F07FCCF58BAD;
	mov.f64 	%fd495, 0d3E5AFD81DA6C3BAF;
	// begin inline asm
	fma.rn.f64 	%fd492, %fd493, %fd488, %fd495;
	// end inline asm
	mov.f64 	%fd499, 0d3E927E55F60F80E6;
	// begin inline asm
	fma.rn.f64 	%fd496, %fd492, %fd488, %fd499;
	// end inline asm
	mov.f64 	%fd503, 0d3EC71DDA8F02D666;
	// begin inline asm
	fma.rn.f64 	%fd500, %fd496, %fd488, %fd503;
	// end inline asm
	mov.f64 	%fd507, 0d3EFA01A013B894E0;
	// begin inline asm
	fma.rn.f64 	%fd504, %fd500, %fd488, %fd507;
	// end inline asm
	mov.f64 	%fd511, 0d3F2A01A01D3AF788;
	// begin inline asm
	fma.rn.f64 	%fd508, %fd504, %fd488, %fd511;
	// end inline asm
	mov.f64 	%fd515, 0d3F56C16C16C3A1EC;
	// begin inline asm
	fma.rn.f64 	%fd512, %fd508, %fd488, %fd515;
	// end inline asm
	mov.f64 	%fd519, 0d3F81111111109161;
	// begin inline asm
	fma.rn.f64 	%fd516, %fd512, %fd488, %fd519;
	// end inline asm
	mov.f64 	%fd523, 0d3FA55555555554C1;
	// begin inline asm
	fma.rn.f64 	%fd520, %fd516, %fd488, %fd523;
	// end inline asm
	mov.f64 	%fd527, 0d3FC555555555556F;
	// begin inline asm
	fma.rn.f64 	%fd524, %fd520, %fd488, %fd527;
	// end inline asm
	// begin inline asm
	fma.rn.f64 	%fd528, %fd524, %fd488, %fd531;
	// end inline asm
	mul.rn.f64 	%fd533, %fd528, %fd488;
	// begin inline asm
	fma.rn.f64 	%fd532, %fd533, %fd488, %fd488;
	// end inline asm
	shl.b32 	%r118, %r117, 20;
	add.s32 	%r119, %r118, 57671680;
	selp.b32 	%r120, %r119, %r118, %p50;
	add.s32 	%r111, %r120, 1071644672;
	mov.u32 	%r110, 0;
	// begin inline asm
	mov.b64 	%fd536, {%r110, %r111};
	// end inline asm
	// begin inline asm
	fma.rn.f64 	%fd537, %fd532, %fd536, %fd536;
	// end inline asm
	mul.rn.f64 	%fd544, %fd537, %fd551;
	neg.f64 	%fd543, %fd544;
	// begin inline asm
	fma.rn.f64 	%fd541, %fd480, %fd543, %fd544;
	// end inline asm
	mul.rn.f64 	%fd1529, %fd545, %fd541;
	bra.uni 	$L__BB0_52;

$L__BB0_49:
	rcp.rn.f64 	%fd416, %fd303;
	mul.rn.f64 	%fd349, %fd416, %fd416;
	mov.f64 	%fd308, 0dC1186DF84479631D;
	mov.f64 	%fd310, 0d41019A6E9A7FFBB8;
	// begin inline asm
	fma.rn.f64 	%fd307, %fd308, %fd349, %fd310;
	// end inline asm
	mov.f64 	%fd314, 0dC0DB040BE3D5CA18;
	// begin inline asm
	fma.rn.f64 	%fd311, %fd307, %fd349, %fd314;
	// end inline asm
	mov.f64 	%fd318, 0d40B012760EE009A0;
	// begin inline asm
	fma.rn.f64 	%fd315, %fd311, %fd349, %fd318;
	// end inline asm
	mov.f64 	%fd322, 0dC082587AE4008D0E;
	// begin inline asm
	fma.rn.f64 	%fd319, %fd315, %fd349, %fd322;
	// end inline asm
	mov.f64 	%fd326, 0d4056DF5D938ACAFE;
	// begin inline asm
	fma.rn.f64 	%fd323, %fd319, %fd349, %fd326;
	// end inline asm
	mov.f64 	%fd330, 0dC030A8D46D765681;
	// begin inline asm
	fma.rn.f64 	%fd327, %fd323, %fd349, %fd330;
	// end inline asm
	mov.f64 	%fd334, 0d400D9EAE0C665C75;
	// begin inline asm
	fma.rn.f64 	%fd331, %fd327, %fd349, %fd334;
	// end inline asm
	mov.f64 	%fd338, 0dBFF0ECF9C8880942;
	// begin inline asm
	fma.rn.f64 	%fd335, %fd331, %fd349, %fd338;
	// end inline asm
	mov.f64 	%fd342, 0d3FDB14C2F82A33F7;
	// begin inline asm
	fma.rn.f64 	%fd339, %fd335, %fd349, %fd342;
	// end inline asm
	mov.f64 	%fd346, 0dBFD20DD75042844F;
	// begin inline asm
	fma.rn.f64 	%fd343, %fd339, %fd349, %fd346;
	// end inline asm
	mov.f64 	%fd350, 0d3FE20DD750429B6B;
	// begin inline asm
	fma.rn.f64 	%fd347, %fd343, %fd349, %fd350;
	// end inline asm
	mul.rn.f64 	%fd417, %fd303, %fd303;
	neg.f64 	%fd358, %fd417;
	// begin inline asm
	fma.rn.f64 	%fd351, %fd303, %fd303, %fd358;
	// end inline asm
	mov.f64 	%fd418, 0d3FF71547652B82FE;
	mul.rn.f64 	%fd419, %fd358, %fd418;
	{
	.reg .b32 %temp; 
	mov.b64 	{%temp, %r101}, %fd419;
	}
	and.b32  	%r102, %r101, -2147483648;
	mov.f64 	%fd402, 0d3FE0000000000000;
	{
	.reg .b32 %temp; 
	mov.b64 	{%temp, %r103}, %fd402;
	}
	or.b32  	%r104, %r103, %r102;
	{
	.reg .b32 %temp; 
	mov.b64 	{%r105, %temp}, %fd402;
	}
	mov.b64 	%fd420, {%r105, %r104};
	add.rz.f64 	%fd421, %fd419, %fd420;
	cvt.rzi.f64.f64 	%fd360, %fd421;
	cvt.rzi.s32.f64 	%r106, %fd360;
	mov.f64 	%fd357, 0dBFE62E42FEFA39EF;
	// begin inline asm
	fma.rn.f64 	%fd355, %fd360, %fd357, %fd358;
	// end inline asm
	mov.f64 	%fd361, 0dBC7ABC9E3B39803F;
	// begin inline asm
	fma.rn.f64 	%fd359, %fd360, %fd361, %fd355;
	// end inline asm
	setp.lt.s32 	%p49, %r106, -1020;
	selp.f64 	%fd422, 0d3C90000000000000, 0d4000000000000000, %p49;
	mov.f64 	%fd364, 0d3E21F07FCCF58BAD;
	mov.f64 	%fd366, 0d3E5AFD81DA6C3BAF;
	// begin inline asm
	fma.rn.f64 	%fd363, %fd364, %fd359, %fd366;
	// end inline asm
	mov.f64 	%fd370, 0d3E927E55F60F80E6;
	// begin inline asm
	fma.rn.f64 	%fd367, %fd363, %fd359, %fd370;
	// end inline asm
	mov.f64 	%fd374, 0d3EC71DDA8F02D666;
	// begin inline asm
	fma.rn.f64 	%fd371, %fd367, %fd359, %fd374;
	// end inline asm
	mov.f64 	%fd378, 0d3EFA01A013B894E0;
	// begin inline asm
	fma.rn.f64 	%fd375, %fd371, %fd359, %fd378;
	// end inline asm
	mov.f64 	%fd382, 0d3F2A01A01D3AF788;
	// begin inline asm
	fma.rn.f64 	%fd379, %fd375, %fd359, %fd382;
	// end inline asm
	mov.f64 	%fd386, 0d3F56C16C16C3A1EC;
	// begin inline asm
	fma.rn.f64 	%fd383, %fd379, %fd359, %fd386;
	// end inline asm
	mov.f64 	%fd390, 0d3F81111111109161;
	// begin inline asm
	fma.rn.f64 	%fd387, %fd383, %fd359, %fd390;
	// end inline asm
	mov.f64 	%fd394, 0d3FA55555555554C1;
	// begin inline asm
	fma.rn.f64 	%fd391, %fd387, %fd359, %fd394;
	// end inline asm
	mov.f64 	%fd398, 0d3FC555555555556F;
	// begin inline asm
	fma.rn.f64 	%fd395, %fd391, %fd359, %fd398;
	// end inline asm
	// begin inline asm
	fma.rn.f64 	%fd399, %fd395, %fd359, %fd402;
	// end inline asm
	mul.rn.f64 	%fd404, %fd399, %fd359;
	// begin inline asm
	fma.rn.f64 	%fd403, %fd404, %fd359, %fd359;
	// end inline asm
	shl.b32 	%r107, %r106, 20;
	add.s32 	%r108, %r107, 57671680;
	selp.b32 	%r109, %r108, %r107, %p49;
	add.s32 	%r100, %r109, 1071644672;
	mov.u32 	%r99, 0;
	// begin inline asm
	mov.b64 	%fd407, {%r99, %r100};
	// end inline asm
	// begin inline asm
	fma.rn.f64 	%fd408, %fd403, %fd407, %fd407;
	// end inline asm
	mul.rn.f64 	%fd415, %fd408, %fd422;
	neg.f64 	%fd414, %fd415;
	// begin inline asm
	fma.rn.f64 	%fd412, %fd351, %fd414, %fd415;
	// end inline asm
	mul.rn.f64 	%fd423, %fd347, %fd416;
	mul.rn.f64 	%fd1529, %fd423, %fd412;

$L__BB0_52:
	fma.rn.f64 	%fd94, %fd1529, 0d3FE0000000000000, 0d0000000000000000;
	mov.f64 	%fd1532, 0d0000000000000000;
	shl.b64 	%rd46, %rd1, 3;
	add.s64 	%rd2, %rd17, %rd46;
	mov.f64 	%fd1533, %fd1532;
	@%p44 bra 	$L__BB0_54;

	ld.global.f64 	%fd640, [%rd2];
	abs.f64 	%fd641, %fd640;
	setp.gtu.f64 	%p53, %fd641, 0d7FF0000000000000;
	add.f64 	%fd642, %fd640, 0d0000000000000000;
	selp.f64 	%fd1532, 0d0000000000000000, %fd642, %p53;
	selp.f64 	%fd1533, 0d0000000000000000, 0d3FF0000000000000, %p53;

$L__BB0_54:
	add.s64 	%rd3, %rd18, %rd46;
	@%p44 bra 	$L__BB0_56;

	ld.global.f64 	%fd643, [%rd3];
	abs.f64 	%fd644, %fd643;
	setp.gtu.f64 	%p55, %fd644, 0d7FF0000000000000;
	add.f64 	%fd645, %fd1532, %fd643;
	selp.f64 	%fd1532, %fd1532, %fd645, %p55;
	add.f64 	%fd646, %fd1533, 0d3FF0000000000000;
	selp.f64 	%fd1533, %fd1533, %fd646, %p55;

$L__BB0_56:
	setp.eq.f64 	%p56, %fd1533, 0d0000000000000000;
	mov.f64 	%fd1558, 0d7FF8000000000214;
	mov.f64 	%fd1557, 0d0000000000000000;
	@%p56 bra 	$L__BB0_213;

	div.rn.f64 	%fd103, %fd1532, %fd1533;
	@%p44 bra 	$L__BB0_134;

	ld.global.f64 	%fd104, [%rd2];
	abs.f64 	%fd105, %fd104;
	setp.gtu.f64 	%p58, %fd105, 0d7FF0000000000000;
	@%p58 bra 	$L__BB0_134;

	setp.lt.f64 	%p59, %fd104, 0d0000000000000000;
	setp.lt.f64 	%p60, %fd103, 0d0000000000000000;
	and.pred  	%p61, %p60, %p59;
	@%p61 bra 	$L__BB0_61;

	setp.leu.f64 	%p62, %fd104, 0d0000000000000000;
	setp.leu.f64 	%p63, %fd103, 0d0000000000000000;
	or.pred  	%p64, %p63, %p62;
	@%p64 bra 	$L__BB0_72;

$L__BB0_61:
	setp.eq.f64 	%p65, %fd104, %fd103;
	mov.f64 	%fd1534, 0d0000000000000000;
	@%p65 bra 	$L__BB0_73;

	setp.eq.f64 	%p66, %fd104, 0d0000000000000000;
	setp.eq.f64 	%p67, %fd103, 0d0000000000000000;
	or.pred  	%p68, %p67, %p66;
	@%p68 bra 	$L__BB0_72;

	sub.f64 	%fd651, %fd104, %fd103;
	abs.f64 	%fd106, %fd651;
	{
	.reg .b32 %temp; 
	mov.b64 	{%temp, %r126}, %fd106;
	}
	and.b32  	%r127, %r126, 2146435072;
	setp.eq.s32 	%p69, %r127, 2146435072;
	mul.f64 	%fd652, %fd105, 0d3D30000000000000;
	setp.gt.f64 	%p70, %fd106, %fd652;
	or.pred  	%p71, %p69, %p70;
	@%p71 bra 	$L__BB0_72;

	abs.f64 	%fd107, %fd103;
	mul.f64 	%fd653, %fd107, 0d3D30000000000000;
	setp.gt.f64 	%p72, %fd106, %fd653;
	@%p72 bra 	$L__BB0_72;

	setp.gtu.f64 	%p73, %fd106, 0d433FFFFFFFFFFFFF;
	@%p73 bra 	$L__BB0_71;

	cvt.rzi.s64.f64 	%rd4, %fd106;
	setp.gt.s64 	%p74, %rd4, 9007199254740991;
	@%p74 bra 	$L__BB0_71;

	cvt.rn.f64.s64 	%fd654, %rd4;
	setp.ne.f64 	%p75, %fd106, %fd654;
	setp.gtu.f64 	%p76, %fd105, 0d433FFFFFFFFFFFFF;
	or.pred  	%p77, %p75, %p76;
	@%p77 bra 	$L__BB0_71;

	cvt.rzi.s64.f64 	%rd5, %fd105;
	setp.gt.s64 	%p78, %rd5, 9007199254740991;
	@%p78 bra 	$L__BB0_71;

	cvt.rn.f64.s64 	%fd655, %rd5;
	setp.ne.f64 	%p79, %fd105, %fd655;
	setp.gtu.f64 	%p80, %fd107, 0d433FFFFFFFFFFFFF;
	or.pred  	%p81, %p79, %p80;
	@%p81 bra 	$L__BB0_71;

	cvt.rzi.s64.f64 	%rd48, %fd107;
	setp.lt.s64 	%p82, %rd48, 9007199254740992;
	cvt.rn.f64.s64 	%fd656, %rd48;
	setp.equ.f64 	%p83, %fd107, %fd656;
	and.pred  	%p84, %p82, %p83;
	@%p84 bra 	$L__BB0_72;

$L__BB0_71:
	mul.f64 	%fd658, %fd105, 0d3CF0000000000000;
	setp.lt.f64 	%p85, %fd106, %fd658;
	mul.f64 	%fd659, %fd107, 0d3CF0000000000000;
	setp.lt.f64 	%p86, %fd106, %fd659;
	and.pred  	%p87, %p85, %p86;
	@%p87 bra 	$L__BB0_73;

$L__BB0_72:
	sub.f64 	%fd1534, %fd104, %fd103;

$L__BB0_73:
	abs.f64 	%fd110, %fd1534;
	setp.gtu.f64 	%p88, %fd110, 0d7FF0000000000000;
	mov.f64 	%fd1544, 0dFFF8000000000000;
	@%p88 bra 	$L__BB0_133;

	setp.eq.f64 	%p89, %fd1534, 0d0000000000000000;
	mov.f64 	%fd1544, 0d0000000000000000;
	mov.f64 	%fd1538, 0d3FF0000000000000;
	@%p89 bra 	$L__BB0_133;

	setp.eq.f64 	%p90, %fd110, 0d3FF0000000000000;
	@%p90 bra 	$L__BB0_101;

	abs.f64 	%fd111, %fd110;
	setp.gtu.f64 	%p91, %fd111, 0d7FF0000000000000;
	@%p91 bra 	$L__BB0_100;
	bra.uni 	$L__BB0_77;

$L__BB0_100:
	add.f64 	%fd1538, %fd110, 0d4000000000000000;
	bra.uni 	$L__BB0_101;

$L__BB0_77:
	setp.eq.f64 	%p92, %fd110, 0d7FF0000000000000;
	@%p92 bra 	$L__BB0_99;
	bra.uni 	$L__BB0_78;

$L__BB0_99:
	mov.f64 	%fd848, 0d4000000000000000;
	{
	.reg .b32 %temp; 
	mov.b64 	{%temp, %r160}, %fd848;
	}
	setp.gt.s32 	%p115, %r160, -1;
	selp.f64 	%fd1538, 0d7FF0000000000000, 0d0000000000000000, %p115;

$L__BB0_101:
	abs.f64 	%fd1465, %fd1534;
	setp.eq.f64 	%p355, %fd1465, 0d3FF0000000000000;
	mov.f64 	%fd850, 0d40F0000000000000;
	mov.f64 	%fd851, 0d0000000000000000;
	mul.rn.f64 	%fd130, %fd851, %fd850;
	setp.eq.f64 	%p116, %fd130, 0d0000000000000000;
	mov.f64 	%fd1543, 0d3FF0000000000000;
	or.pred  	%p118, %p355, %p116;
	@%p118 bra 	$L__BB0_132;

	abs.f64 	%fd131, %fd110;
	setp.gtu.f64 	%p119, %fd131, 0d7FF0000000000000;
	@%p119 bra 	$L__BB0_131;

	abs.f64 	%fd132, %fd130;
	setp.gtu.f64 	%p120, %fd132, 0d7FF0000000000000;
	@%p120 bra 	$L__BB0_131;
	bra.uni 	$L__BB0_104;

$L__BB0_131:
	add.f64 	%fd1543, %fd110, %fd130;

$L__BB0_132:
	mul.rn.f64 	%fd1544, %fd1538, %fd1543;

$L__BB0_133:
	add.f64 	%fd1557, %fd1544, 0d0000000000000000;

$L__BB0_134:
	@%p44 bra 	$L__BB0_211;

	ld.global.f64 	%fd163, [%rd3];
	abs.f64 	%fd164, %fd163;
	setp.gtu.f64 	%p150, %fd164, 0d7FF0000000000000;
	@%p150 bra 	$L__BB0_211;

	setp.lt.f64 	%p151, %fd163, 0d0000000000000000;
	setp.lt.f64 	%p152, %fd103, 0d0000000000000000;
	and.pred  	%p153, %p152, %p151;
	@%p153 bra 	$L__BB0_138;

	setp.leu.f64 	%p154, %fd163, 0d0000000000000000;
	setp.leu.f64 	%p155, %fd103, 0d0000000000000000;
	or.pred  	%p156, %p155, %p154;
	@%p156 bra 	$L__BB0_149;

$L__BB0_138:
	setp.eq.f64 	%p157, %fd163, %fd103;
	mov.f64 	%fd168, 0d0000000000000000;
	@%p157 bra 	$L__BB0_150;

	setp.eq.f64 	%p158, %fd163, 0d0000000000000000;
	setp.eq.f64 	%p159, %fd103, 0d0000000000000000;
	or.pred  	%p160, %p159, %p158;
	@%p160 bra 	$L__BB0_149;

	sub.f64 	%fd1034, %fd163, %fd103;
	abs.f64 	%fd165, %fd1034;
	{
	.reg .b32 %temp; 
	mov.b64 	{%temp, %r194}, %fd165;
	}
	and.b32  	%r195, %r194, 2146435072;
	setp.eq.s32 	%p161, %r195, 2146435072;
	mul.f64 	%fd1035, %fd164, 0d3D30000000000000;
	setp.gt.f64 	%p162, %fd165, %fd1035;
	or.pred  	%p163, %p161, %p162;
	@%p163 bra 	$L__BB0_149;

	abs.f64 	%fd166, %fd103;
	mul.f64 	%fd1036, %fd166, 0d3D30000000000000;
	setp.gt.f64 	%p164, %fd165, %fd1036;
	@%p164 bra 	$L__BB0_149;

	setp.gtu.f64 	%p165, %fd165, 0d433FFFFFFFFFFFFF;
	@%p165 bra 	$L__BB0_148;

	cvt.rzi.s64.f64 	%rd6, %fd165;
	setp.gt.s64 	%p166, %rd6, 9007199254740991;
	@%p166 bra 	$L__BB0_148;

	cvt.rn.f64.s64 	%fd1037, %rd6;
	setp.ne.f64 	%p167, %fd165, %fd1037;
	setp.gtu.f64 	%p168, %fd164, 0d433FFFFFFFFFFFFF;
	or.pred  	%p169, %p167, %p168;
	@%p169 bra 	$L__BB0_148;

	cvt.rzi.s64.f64 	%rd7, %fd164;
	setp.gt.s64 	%p170, %rd7, 9007199254740991;
	@%p170 bra 	$L__BB0_148;

	cvt.rn.f64.s64 	%fd1038, %rd7;
	setp.ne.f64 	%p171, %fd164, %fd1038;
	setp.gtu.f64 	%p172, %fd166, 0d433FFFFFFFFFFFFF;
	or.pred  	%p173, %p171, %p172;
	@%p173 bra 	$L__BB0_148;

	cvt.rzi.s64.f64 	%rd55, %fd166;
	setp.lt.s64 	%p174, %rd55, 9007199254740992;
	cvt.rn.f64.s64 	%fd1039, %rd55;
	setp.equ.f64 	%p175, %fd166, %fd1039;
	and.pred  	%p176, %p174, %p175;
	@%p176 bra 	$L__BB0_149;

$L__BB0_148:
	mul.f64 	%fd1041, %fd164, 0d3CF0000000000000;
	setp.lt.f64 	%p177, %fd165, %fd1041;
	mul.f64 	%fd1042, %fd166, 0d3CF0000000000000;
	setp.lt.f64 	%p178, %fd165, %fd1042;
	and.pred  	%p179, %p177, %p178;
	@%p179 bra 	$L__BB0_150;

$L__BB0_149:
	sub.f64 	%fd168, %fd163, %fd103;

$L__BB0_150:
	abs.f64 	%fd169, %fd168;
	setp.gtu.f64 	%p180, %fd169, 0d7FF0000000000000;
	mov.f64 	%fd1556, 0dFFF8000000000000;
	@%p180 bra 	$L__BB0_210;

	setp.eq.f64 	%p181, %fd168, 0d0000000000000000;
	mov.f64 	%fd1556, 0d0000000000000000;
	mov.f64 	%fd1550, 0d3FF0000000000000;
	@%p181 bra 	$L__BB0_210;

	setp.eq.f64 	%p182, %fd169, 0d3FF0000000000000;
	@%p182 bra 	$L__BB0_178;

	abs.f64 	%fd170, %fd169;
	setp.gtu.f64 	%p183, %fd170, 0d7FF0000000000000;
	@%p183 bra 	$L__BB0_177;
	bra.uni 	$L__BB0_154;

$L__BB0_177:
	add.f64 	%fd1550, %fd169, 0d4000000000000000;
	bra.uni 	$L__BB0_178;

$L__BB0_154:
	setp.eq.f64 	%p184, %fd169, 0d7FF0000000000000;
	@%p184 bra 	$L__BB0_176;
	bra.uni 	$L__BB0_155;

$L__BB0_176:
	mov.f64 	%fd1231, 0d4000000000000000;
	{
	.reg .b32 %temp; 
	mov.b64 	{%temp, %r228}, %fd1231;
	}
	setp.gt.s32 	%p207, %r228, -1;
	selp.f64 	%fd1550, 0d7FF0000000000000, 0d0000000000000000, %p207;

$L__BB0_178:
	abs.f64 	%fd1471, %fd168;
	setp.eq.f64 	%p357, %fd1471, 0d3FF0000000000000;
	mov.f64 	%fd1233, 0d40F0000000000000;
	mov.f64 	%fd1234, 0d0000000000000000;
	mul.rn.f64 	%fd189, %fd1234, %fd1233;
	setp.eq.f64 	%p208, %fd189, 0d0000000000000000;
	mov.f64 	%fd1555, 0d3FF0000000000000;
	or.pred  	%p210, %p357, %p208;
	@%p210 bra 	$L__BB0_209;

	abs.f64 	%fd1472, %fd168;
	abs.f64 	%fd190, %fd1472;
	setp.gtu.f64 	%p211, %fd190, 0d7FF0000000000000;
	@%p211 bra 	$L__BB0_208;

	abs.f64 	%fd191, %fd189;
	setp.gtu.f64 	%p212, %fd191, 0d7FF0000000000000;
	@%p212 bra 	$L__BB0_208;
	bra.uni 	$L__BB0_181;

$L__BB0_208:
	abs.f64 	%fd1479, %fd168;
	add.f64 	%fd1555, %fd1479, %fd189;

$L__BB0_209:
	mul.rn.f64 	%fd1556, %fd1550, %fd1555;

$L__BB0_210:
	add.f64 	%fd1557, %fd1557, %fd1556;

$L__BB0_211:
	mov.f64 	%fd1558, 0d7FF8000000000214;
	setp.le.f64 	%p241, %fd1533, 0d3FF0000000000000;
	@%p241 bra 	$L__BB0_213;

	add.f64 	%fd1417, %fd1533, 0dBFF0000000000000;
	div.rn.f64 	%fd1558, %fd1557, %fd1417;

$L__BB0_213:
	setp.gt.f64 	%p242, %fd94, 0d0000000000000000;
	setp.lt.f64 	%p243, %fd1558, 0d0000000000000000;
	and.pred  	%p244, %p242, %p243;
	@%p244 bra 	$L__BB0_215;

	setp.geu.f64 	%p245, %fd94, 0d0000000000000000;
	setp.leu.f64 	%p246, %fd1558, 0d0000000000000000;
	or.pred  	%p247, %p245, %p246;
	@%p247 bra 	$L__BB0_227;

$L__BB0_215:
	neg.f64 	%fd224, %fd94;
	setp.eq.f64 	%p248, %fd1558, %fd224;
	mov.f64 	%fd1559, 0d0000000000000000;
	@%p248 bra 	$L__BB0_228;

	setp.eq.f64 	%p249, %fd1558, 0d0000000000000000;
	setp.eq.f64 	%p250, %fd94, 0d8000000000000000;
	or.pred  	%p251, %p250, %p249;
	@%p251 bra 	$L__BB0_227;

	add.f64 	%fd1419, %fd94, %fd1558;
	abs.f64 	%fd225, %fd1419;
	{
	.reg .b32 %temp; 
	mov.b64 	{%temp, %r262}, %fd225;
	}
	and.b32  	%r263, %r262, 2146435072;
	setp.eq.s32 	%p252, %r263, 2146435072;
	@%p252 bra 	$L__BB0_227;

	abs.f64 	%fd226, %fd1558;
	mul.f64 	%fd1420, %fd226, 0d3D30000000000000;
	setp.gt.f64 	%p253, %fd225, %fd1420;
	@%p253 bra 	$L__BB0_227;

	abs.f64 	%fd227, %fd224;
	mul.f64 	%fd1421, %fd227, 0d3D30000000000000;
	setp.gt.f64 	%p254, %fd225, %fd1421;
	@%p254 bra 	$L__BB0_227;

	setp.gtu.f64 	%p255, %fd225, 0d433FFFFFFFFFFFFF;
	@%p255 bra 	$L__BB0_226;

	cvt.rzi.s64.f64 	%rd8, %fd225;
	setp.gt.s64 	%p256, %rd8, 9007199254740991;
	@%p256 bra 	$L__BB0_226;

	cvt.rn.f64.s64 	%fd1422, %rd8;
	setp.ne.f64 	%p257, %fd225, %fd1422;
	setp.gtu.f64 	%p258, %fd226, 0d433FFFFFFFFFFFFF;
	or.pred  	%p259, %p257, %p258;
	@%p259 bra 	$L__BB0_226;

	cvt.rzi.s64.f64 	%rd9, %fd226;
	setp.gt.s64 	%p260, %rd9, 9007199254740991;
	@%p260 bra 	$L__BB0_226;

	cvt.rn.f64.s64 	%fd1423, %rd9;
	setp.ne.f64 	%p261, %fd226, %fd1423;
	setp.gtu.f64 	%p262, %fd227, 0d433FFFFFFFFFFFFF;
	or.pred  	%p263, %p261, %p262;
	@%p263 bra 	$L__BB0_226;

	cvt.rzi.s64.f64 	%rd62, %fd227;
	setp.lt.s64 	%p264, %rd62, 9007199254740992;
	cvt.rn.f64.s64 	%fd1424, %rd62;
	setp.equ.f64 	%p265, %fd227, %fd1424;
	and.pred  	%p266, %p264, %p265;
	@%p266 bra 	$L__BB0_227;

$L__BB0_226:
	mul.f64 	%fd1426, %fd226, 0d3CF0000000000000;
	setp.lt.f64 	%p267, %fd225, %fd1426;
	mul.f64 	%fd1427, %fd227, 0d3CF0000000000000;
	setp.lt.f64 	%p268, %fd225, %fd1427;
	and.pred  	%p269, %p267, %p268;
	@%p269 bra 	$L__BB0_228;

$L__BB0_227:
	add.f64 	%fd1559, %fd94, %fd1558;

$L__BB0_228:
	setp.lt.f64 	%p270, %fd1527, 0d0000000000000000;
	setp.lt.f64 	%p271, %fd1559, 0d0000000000000000;
	and.pred  	%p272, %p270, %p271;
	@%p272 bra 	$L__BB0_230;

	setp.leu.f64 	%p273, %fd1559, 0d0000000000000000;
	setp.leu.f64 	%p274, %fd1527, 0d0000000000000000;
	or.pred  	%p275, %p274, %p273;
	@%p275 bra 	$L__BB0_242;

$L__BB0_230:
	setp.eq.f64 	%p276, %fd1559, %fd1527;
	mov.f64 	%fd1560, 0d0000000000000000;
	@%p276 bra 	$L__BB0_243;

	setp.eq.f64 	%p277, %fd1559, 0d0000000000000000;
	setp.eq.f64 	%p278, %fd1527, 0d0000000000000000;
	or.pred  	%p279, %p278, %p277;
	@%p279 bra 	$L__BB0_242;

	sub.f64 	%fd1429, %fd1559, %fd1527;
	abs.f64 	%fd230, %fd1429;
	{
	.reg .b32 %temp; 
	mov.b64 	{%temp, %r264}, %fd230;
	}
	and.b32  	%r265, %r264, 2146435072;
	setp.eq.s32 	%p280, %r265, 2146435072;
	@%p280 bra 	$L__BB0_242;

	abs.f64 	%fd231, %fd1559;
	mul.f64 	%fd1430, %fd231, 0d3D30000000000000;
	setp.gt.f64 	%p281, %fd230, %fd1430;
	@%p281 bra 	$L__BB0_242;

	abs.f64 	%fd232, %fd1527;
	mul.f64 	%fd1431, %fd232, 0d3D30000000000000;
	setp.gt.f64 	%p282, %fd230, %fd1431;
	@%p282 bra 	$L__BB0_242;

	setp.gtu.f64 	%p283, %fd230, 0d433FFFFFFFFFFFFF;
	@%p283 bra 	$L__BB0_241;

	cvt.rzi.s64.f64 	%rd10, %fd230;
	setp.gt.s64 	%p284, %rd10, 9007199254740991;
	@%p284 bra 	$L__BB0_241;

	cvt.rn.f64.s64 	%fd1432, %rd10;
	setp.ne.f64 	%p285, %fd230, %fd1432;
	setp.gtu.f64 	%p286, %fd231, 0d433FFFFFFFFFFFFF;
	or.pred  	%p287, %p285, %p286;
	@%p287 bra 	$L__BB0_241;

	cvt.rzi.s64.f64 	%rd11, %fd231;
	setp.gt.s64 	%p288, %rd11, 9007199254740991;
	@%p288 bra 	$L__BB0_241;

	cvt.rn.f64.s64 	%fd1433, %rd11;
	setp.ne.f64 	%p289, %fd231, %fd1433;
	setp.gtu.f64 	%p290, %fd232, 0d433FFFFFFFFFFFFF;
	or.pred  	%p291, %p289, %p290;
	@%p291 bra 	$L__BB0_241;

	cvt.rzi.s64.f64 	%rd63, %fd232;
	setp.lt.s64 	%p292, %rd63, 9007199254740992;
	cvt.rn.f64.s64 	%fd1434, %rd63;
	setp.equ.f64 	%p293, %fd232, %fd1434;
	and.pred  	%p294, %p292, %p293;
	@%p294 bra 	$L__BB0_242;

$L__BB0_241:
	mul.f64 	%fd1436, %fd231, 0d3CF0000000000000;
	setp.lt.f64 	%p295, %fd230, %fd1436;
	mul.f64 	%fd1437, %fd232, 0d3CF0000000000000;
	setp.lt.f64 	%p296, %fd230, %fd1437;
	and.pred  	%p297, %p295, %p296;
	@%p297 bra 	$L__BB0_243;

$L__BB0_242:
	sub.f64 	%fd1560, %fd1559, %fd1527;

$L__BB0_243:
	setp.gt.f64 	%p298, %fd62, 0d0000000000000000;
	setp.lt.f64 	%p299, %fd1560, 0d0000000000000000;
	and.pred  	%p300, %p298, %p299;
	@%p300 bra 	$L__BB0_245;

	setp.geu.f64 	%p301, %fd62, 0d0000000000000000;
	setp.leu.f64 	%p302, %fd1560, 0d0000000000000000;
	or.pred  	%p303, %p301, %p302;
	@%p303 bra 	$L__BB0_257;

$L__BB0_245:
	neg.f64 	%fd235, %fd62;
	setp.eq.f64 	%p304, %fd1560, %fd235;
	mov.f64 	%fd1561, 0d0000000000000000;
	@%p304 bra 	$L__BB0_258;

	setp.eq.f64 	%p305, %fd1560, 0d0000000000000000;
	setp.eq.f64 	%p306, %fd62, 0d8000000000000000;
	or.pred  	%p307, %p306, %p305;
	@%p307 bra 	$L__BB0_257;

	add.f64 	%fd1439, %fd62, %fd1560;
	abs.f64 	%fd236, %fd1439;
	{
	.reg .b32 %temp; 
	mov.b64 	{%temp, %r266}, %fd236;
	}
	and.b32  	%r267, %r266, 2146435072;
	setp.eq.s32 	%p308, %r267, 2146435072;
	@%p308 bra 	$L__BB0_257;

	abs.f64 	%fd237, %fd1560;
	mul.f64 	%fd1440, %fd237, 0d3D30000000000000;
	setp.gt.f64 	%p309, %fd236, %fd1440;
	@%p309 bra 	$L__BB0_257;

	abs.f64 	%fd238, %fd235;
	mul.f64 	%fd1441, %fd238, 0d3D30000000000000;
	setp.gt.f64 	%p310, %fd236, %fd1441;
	@%p310 bra 	$L__BB0_257;

	setp.gtu.f64 	%p311, %fd236, 0d433FFFFFFFFFFFFF;
	@%p311 bra 	$L__BB0_256;

	cvt.rzi.s64.f64 	%rd12, %fd236;
	setp.gt.s64 	%p312, %rd12, 9007199254740991;
	@%p312 bra 	$L__BB0_256;

	cvt.rn.f64.s64 	%fd1442, %rd12;
	setp.ne.f64 	%p313, %fd236, %fd1442;
	setp.gtu.f64 	%p314, %fd237, 0d433FFFFFFFFFFFFF;
	or.pred  	%p315, %p313, %p314;
	@%p315 bra 	$L__BB0_256;

	cvt.rzi.s64.f64 	%rd13, %fd237;
	setp.gt.s64 	%p316, %rd13, 9007199254740991;
	@%p316 bra 	$L__BB0_256;

	cvt.rn.f64.s64 	%fd1443, %rd13;
	setp.ne.f64 	%p317, %fd237, %fd1443;
	setp.gtu.f64 	%p318, %fd238, 0d433FFFFFFFFFFFFF;
	or.pred  	%p319, %p317, %p318;
	@%p319 bra 	$L__BB0_256;

	cvt.rzi.s64.f64 	%rd64, %fd238;
	setp.lt.s64 	%p320, %rd64, 9007199254740992;
	cvt.rn.f64.s64 	%fd1444, %rd64;
	setp.equ.f64 	%p321, %fd238, %fd1444;
	and.pred  	%p322, %p320, %p321;
	@%p322 bra 	$L__BB0_257;

$L__BB0_256:
	mul.f64 	%fd1446, %fd237, 0d3CF0000000000000;
	setp.lt.f64 	%p323, %fd236, %fd1446;
	mul.f64 	%fd1447, %fd238, 0d3CF0000000000000;
	setp.lt.f64 	%p324, %fd236, %fd1447;
	and.pred  	%p325, %p323, %p324;
	@%p325 bra 	$L__BB0_258;

$L__BB0_257:
	add.f64 	%fd1561, %fd62, %fd1560;

$L__BB0_258:
	setp.gt.f64 	%p326, %fd29, 0d0000000000000000;
	setp.lt.f64 	%p327, %fd1561, 0d0000000000000000;
	and.pred  	%p328, %p326, %p327;
	@%p328 bra 	$L__BB0_260;

	setp.geu.f64 	%p329, %fd29, 0d0000000000000000;
	setp.leu.f64 	%p330, %fd1561, 0d0000000000000000;
	or.pred  	%p331, %p329, %p330;
	@%p331 bra 	$L__BB0_272;

$L__BB0_260:
	neg.f64 	%fd241, %fd29;
	setp.eq.f64 	%p332, %fd1561, %fd241;
	mov.f64 	%fd1562, 0d0000000000000000;
	@%p332 bra 	$L__BB0_273;

	setp.eq.f64 	%p333, %fd1561, 0d0000000000000000;
	setp.eq.f64 	%p334, %fd29, 0d8000000000000000;
	or.pred  	%p335, %p334, %p333;
	@%p335 bra 	$L__BB0_272;

	add.f64 	%fd1449, %fd29, %fd1561;
	abs.f64 	%fd242, %fd1449;
	{
	.reg .b32 %temp; 
	mov.b64 	{%temp, %r268}, %fd242;
	}
	and.b32  	%r269, %r268, 2146435072;
	setp.eq.s32 	%p336, %r269, 2146435072;
	@%p336 bra 	$L__BB0_272;

	abs.f64 	%fd243, %fd1561;
	mul.f64 	%fd1450, %fd243, 0d3D30000000000000;
	setp.gt.f64 	%p337, %fd242, %fd1450;
	@%p337 bra 	$L__BB0_272;

	abs.f64 	%fd244, %fd241;
	mul.f64 	%fd1451, %fd244, 0d3D30000000000000;
	setp.gt.f64 	%p338, %fd242, %fd1451;
	@%p338 bra 	$L__BB0_272;

	setp.gtu.f64 	%p339, %fd242, 0d433FFFFFFFFFFFFF;
	@%p339 bra 	$L__BB0_271;

	cvt.rzi.s64.f64 	%rd14, %fd242;
	setp.gt.s64 	%p340, %rd14, 9007199254740991;
	@%p340 bra 	$L__BB0_271;

	cvt.rn.f64.s64 	%fd1452, %rd14;
	setp.ne.f64 	%p341, %fd242, %fd1452;
	setp.gtu.f64 	%p342, %fd243, 0d433FFFFFFFFFFFFF;
	or.pred  	%p343, %p341, %p342;
	@%p343 bra 	$L__BB0_271;

	cvt.rzi.s64.f64 	%rd15, %fd243;
	setp.gt.s64 	%p344, %rd15, 9007199254740991;
	@%p344 bra 	$L__BB0_271;

	cvt.rn.f64.s64 	%fd1453, %rd15;
	setp.ne.f64 	%p345, %fd243, %fd1453;
	setp.gtu.f64 	%p346, %fd244, 0d433FFFFFFFFFFFFF;
	or.pred  	%p347, %p345, %p346;
	@%p347 bra 	$L__BB0_271;

	cvt.rzi.s64.f64 	%rd65, %fd244;
	setp.lt.s64 	%p348, %rd65, 9007199254740992;
	cvt.rn.f64.s64 	%fd1454, %rd65;
	setp.equ.f64 	%p349, %fd244, %fd1454;
	and.pred  	%p350, %p348, %p349;
	@%p350 bra 	$L__BB0_272;

$L__BB0_271:
	mul.f64 	%fd1456, %fd243, 0d3CF0000000000000;
	setp.lt.f64 	%p351, %fd242, %fd1456;
	mul.f64 	%fd1457, %fd244, 0d3CF0000000000000;
	setp.lt.f64 	%p352, %fd242, %fd1457;
	and.pred  	%p353, %p351, %p352;
	@%p353 bra 	$L__BB0_273;

$L__BB0_272:
	add.f64 	%fd1562, %fd29, %fd1561;

$L__BB0_273:
	mov.b32 	%r276, %envreg3;
	mov.u32 	%r275, %tid.x;
	add.s32 	%r274, %r275, %r276;
	mov.u32 	%r273, %ctaid.x;
	mov.u32 	%r272, %ntid.x;
	mad.lo.s32 	%r271, %r272, %r273, %r274;
	cvt.s64.s32 	%rd70, %r271;
	shl.b64 	%rd69, %rd70, 3;
	ld.param.u64 	%rd68, [DynamicKernel_nop_fsum_fsum_fsub_fsum_Var_OpNormsdist_Covar_OpPearson_Slope_param_0];
	add.s64 	%rd67, %rd68, %rd69;
	st.global.f64 	[%rd67], %fd1562;
	ret;

$L__BB0_104:
	setp.eq.f64 	%p121, %fd110, 0d7FF0000000000000;
	@%p121 bra 	$L__BB0_130;
	bra.uni 	$L__BB0_105;

$L__BB0_130:
	{
	.reg .b32 %temp; 
	mov.b64 	{%temp, %r193}, %fd130;
	}
	setp.gt.s32 	%p148, %r193, -1;
	selp.f64 	%fd1543, 0d7FF0000000000000, 0d0000000000000000, %p148;
	bra.uni 	$L__BB0_132;

$L__BB0_181:
	abs.f64 	%fd1473, %fd168;
	setp.eq.f64 	%p213, %fd1473, 0d7FF0000000000000;
	@%p213 bra 	$L__BB0_207;
	bra.uni 	$L__BB0_182;

$L__BB0_207:
	{
	.reg .b32 %temp; 
	mov.b64 	{%temp, %r261}, %fd189;
	}
	setp.gt.s32 	%p240, %r261, -1;
	selp.f64 	%fd1555, 0d7FF0000000000000, 0d0000000000000000, %p240;
	bra.uni 	$L__BB0_209;

$L__BB0_78:
	mov.f64 	%fd663, 0d4000000000000000;
	{
	.reg .b32 %temp; 
	mov.b64 	{%r128, %temp}, %fd663;
	}
	{
	.reg .b32 %temp; 
	mov.b64 	{%temp, %r129}, %fd663;
	}
	and.b32  	%r130, %r129, 2147483647;
	setp.ne.s32 	%p93, %r130, 2146435072;
	setp.ne.s32 	%p94, %r128, 0;
	or.pred  	%p95, %p94, %p93;
	@%p95 bra 	$L__BB0_81;
	bra.uni 	$L__BB0_79;

$L__BB0_81:
	mov.f64 	%fd666, 0d3FE0000000000000;
	mul.rn.f64 	%fd667, %fd666, %fd663;
	cvt.rzi.f64.f64 	%fd668, %fd667;
	mul.rn.f64 	%fd669, %fd663, %fd668;
	sub.f64 	%fd670, %fd663, %fd669;
	abs.f64 	%fd113, %fd670;
	setp.eq.f64 	%p98, %fd110, 0d0000000000000000;
	@%p98 bra 	$L__BB0_98;
	bra.uni 	$L__BB0_82;

$L__BB0_98:
	setp.eq.f64 	%p114, %fd113, 0d3FF0000000000000;
	selp.f64 	%fd1538, %fd110, 0d0000000000000000, %p114;
	bra.uni 	$L__BB0_101;

$L__BB0_155:
	mov.f64 	%fd1046, 0d4000000000000000;
	{
	.reg .b32 %temp; 
	mov.b64 	{%r196, %temp}, %fd1046;
	}
	{
	.reg .b32 %temp; 
	mov.b64 	{%temp, %r197}, %fd1046;
	}
	and.b32  	%r198, %r197, 2147483647;
	setp.ne.s32 	%p185, %r198, 2146435072;
	setp.ne.s32 	%p186, %r196, 0;
	or.pred  	%p187, %p186, %p185;
	@%p187 bra 	$L__BB0_158;
	bra.uni 	$L__BB0_156;

$L__BB0_158:
	mov.f64 	%fd1049, 0d3FE0000000000000;
	mul.rn.f64 	%fd1050, %fd1049, %fd1046;
	cvt.rzi.f64.f64 	%fd1051, %fd1050;
	mul.rn.f64 	%fd1052, %fd1046, %fd1051;
	sub.f64 	%fd1053, %fd1046, %fd1052;
	abs.f64 	%fd172, %fd1053;
	setp.eq.f64 	%p190, %fd169, 0d0000000000000000;
	@%p190 bra 	$L__BB0_175;
	bra.uni 	$L__BB0_159;

$L__BB0_175:
	setp.eq.f64 	%p206, %fd172, 0d3FF0000000000000;
	selp.f64 	%fd1550, %fd169, 0d0000000000000000, %p206;
	bra.uni 	$L__BB0_178;

$L__BB0_105:
	{
	.reg .b32 %temp; 
	mov.b64 	{%r161, %temp}, %fd130;
	}
	{
	.reg .b32 %temp; 
	mov.b64 	{%temp, %r162}, %fd130;
	}
	and.b32  	%r163, %r162, 2147483647;
	setp.ne.s32 	%p122, %r163, 2146435072;
	setp.ne.s32 	%p123, %r161, 0;
	or.pred  	%p124, %p123, %p122;
	@%p124 bra 	$L__BB0_109;
	bra.uni 	$L__BB0_106;

$L__BB0_109:
	mov.f64 	%fd853, 0d3FE0000000000000;
	mul.rn.f64 	%fd854, %fd853, %fd130;
	cvt.rzi.f64.f64 	%fd855, %fd854;
	mov.f64 	%fd856, 0d4000000000000000;
	mul.rn.f64 	%fd857, %fd856, %fd855;
	sub.f64 	%fd858, %fd130, %fd857;
	abs.f64 	%fd135, %fd858;
	setp.eq.f64 	%p128, %fd110, 0d0000000000000000;
	@%p128 bra 	$L__BB0_128;
	bra.uni 	$L__BB0_110;

$L__BB0_128:
	setp.eq.f64 	%p146, %fd135, 0d3FF0000000000000;
	selp.f64 	%fd1543, %fd110, 0d0000000000000000, %p146;
	setp.geu.f64 	%p147, %fd130, 0d0000000000000000;
	@%p147 bra 	$L__BB0_132;

	rcp.rn.f64 	%fd1543, %fd1543;
	bra.uni 	$L__BB0_132;

$L__BB0_182:
	{
	.reg .b32 %temp; 
	mov.b64 	{%r229, %temp}, %fd189;
	}
	{
	.reg .b32 %temp; 
	mov.b64 	{%temp, %r230}, %fd189;
	}
	and.b32  	%r231, %r230, 2147483647;
	setp.ne.s32 	%p214, %r231, 2146435072;
	setp.ne.s32 	%p215, %r229, 0;
	or.pred  	%p216, %p215, %p214;
	@%p216 bra 	$L__BB0_186;
	bra.uni 	$L__BB0_183;

$L__BB0_186:
	abs.f64 	%fd1475, %fd168;
	mov.f64 	%fd1236, 0d3FE0000000000000;
	mul.rn.f64 	%fd1237, %fd1236, %fd189;
	cvt.rzi.f64.f64 	%fd1238, %fd1237;
	mov.f64 	%fd1239, 0d4000000000000000;
	mul.rn.f64 	%fd1240, %fd1239, %fd1238;
	sub.f64 	%fd1241, %fd189, %fd1240;
	abs.f64 	%fd194, %fd1241;
	setp.eq.f64 	%p220, %fd1475, 0d0000000000000000;
	@%p220 bra 	$L__BB0_205;
	bra.uni 	$L__BB0_187;

$L__BB0_205:
	abs.f64 	%fd1478, %fd168;
	setp.eq.f64 	%p238, %fd194, 0d3FF0000000000000;
	selp.f64 	%fd1555, %fd1478, 0d0000000000000000, %p238;
	setp.geu.f64 	%p239, %fd189, 0d0000000000000000;
	@%p239 bra 	$L__BB0_209;

	rcp.rn.f64 	%fd1555, %fd1555;
	bra.uni 	$L__BB0_209;

$L__BB0_79:
	setp.eq.f64 	%p96, %fd110, 0dBFF0000000000000;
	@%p96 bra 	$L__BB0_101;

	setp.gt.f64 	%p97, %fd111, 0d3FF0000000000000;
	selp.f64 	%fd1538, 0d7FF0000000000000, 0d0000000000000000, %p97;
	bra.uni 	$L__BB0_101;

$L__BB0_156:
	setp.eq.f64 	%p188, %fd169, 0dBFF0000000000000;
	@%p188 bra 	$L__BB0_178;

	setp.gt.f64 	%p189, %fd170, 0d3FF0000000000000;
	selp.f64 	%fd1550, 0d7FF0000000000000, 0d0000000000000000, %p189;
	bra.uni 	$L__BB0_178;

$L__BB0_106:
	setp.eq.f64 	%p125, %fd110, 0dBFF0000000000000;
	@%p125 bra 	$L__BB0_132;

	setp.gt.f64 	%p126, %fd131, 0d3FF0000000000000;
	selp.f64 	%fd1543, 0d7FF0000000000000, 0d0000000000000000, %p126;
	setp.geu.f64 	%p127, %fd130, 0d0000000000000000;
	@%p127 bra 	$L__BB0_132;

	rcp.rn.f64 	%fd1543, %fd1543;
	bra.uni 	$L__BB0_132;

$L__BB0_183:
	abs.f64 	%fd1474, %fd168;
	setp.eq.f64 	%p217, %fd1474, 0dBFF0000000000000;
	@%p217 bra 	$L__BB0_209;

	setp.gt.f64 	%p218, %fd190, 0d3FF0000000000000;
	selp.f64 	%fd1555, 0d7FF0000000000000, 0d0000000000000000, %p218;
	setp.geu.f64 	%p219, %fd189, 0d0000000000000000;
	@%p219 bra 	$L__BB0_209;

	rcp.rn.f64 	%fd1555, %fd1555;
	bra.uni 	$L__BB0_209;

$L__BB0_82:
	setp.eq.f64 	%p99, %fd110, 0dFFF0000000000000;
	@%p99 bra 	$L__BB0_96;
	bra.uni 	$L__BB0_83;

$L__BB0_96:
	setp.neu.f64 	%p113, %fd113, 0d3FF0000000000000;
	mov.f64 	%fd1538, 0d7FF0000000000000;
	@%p113 bra 	$L__BB0_101;

	mov.f64 	%fd1538, 0dFFF0000000000000;
	bra.uni 	$L__BB0_101;

$L__BB0_159:
	setp.eq.f64 	%p191, %fd169, 0dFFF0000000000000;
	@%p191 bra 	$L__BB0_173;
	bra.uni 	$L__BB0_160;

$L__BB0_173:
	setp.neu.f64 	%p205, %fd172, 0d3FF0000000000000;
	mov.f64 	%fd1550, 0d7FF0000000000000;
	@%p205 bra 	$L__BB0_178;

	mov.f64 	%fd1550, 0dFFF0000000000000;
	bra.uni 	$L__BB0_178;

$L__BB0_110:
	setp.eq.f64 	%p129, %fd110, 0dFFF0000000000000;
	@%p129 bra 	$L__BB0_126;
	bra.uni 	$L__BB0_111;

$L__BB0_126:
	setp.neu.f64 	%p144, %fd135, 0d3FF0000000000000;
	setp.lt.f64 	%p145, %fd130, 0d0000000000000000;
	selp.f64 	%fd1543, 0d0000000000000000, 0d7FF0000000000000, %p145;
	@%p144 bra 	$L__BB0_132;

	mov.b64 	%rd53, %fd1543;
	xor.b64  	%rd54, %rd53, -9223372036854775808;
	mov.b64 	%fd1543, %rd54;
	bra.uni 	$L__BB0_132;

$L__BB0_187:
	abs.f64 	%fd1476, %fd168;
	setp.eq.f64 	%p221, %fd1476, 0dFFF0000000000000;
	@%p221 bra 	$L__BB0_203;
	bra.uni 	$L__BB0_188;

$L__BB0_203:
	setp.neu.f64 	%p236, %fd194, 0d3FF0000000000000;
	setp.lt.f64 	%p237, %fd189, 0d0000000000000000;
	selp.f64 	%fd1555, 0d0000000000000000, 0d7FF0000000000000, %p237;
	@%p236 bra 	$L__BB0_209;

	mov.b64 	%rd60, %fd1555;
	xor.b64  	%rd61, %rd60, -9223372036854775808;
	mov.b64 	%fd1555, %rd61;
	bra.uni 	$L__BB0_209;

$L__BB0_83:
	setp.geu.f64 	%p100, %fd110, 0d0000000000000000;
	@%p100 bra 	$L__BB0_85;

	mov.f64 	%fd672, 0d4000000000000000;
	cvt.rzi.f64.f64 	%fd673, %fd672;
	setp.neu.f64 	%p101, %fd673, 0d4000000000000000;
	mov.f64 	%fd1538, 0dFFF8000000000000;
	@%p101 bra 	$L__BB0_101;

$L__BB0_85:
	// begin inline asm
	{ 
	.reg 	.b32 lo; 
	mov.b64 	{lo, %r291}, %fd111; 
	}
	// end inline asm
	// begin inline asm
	{ 
	.reg 	.b32 hi; 
	mov.b64 	{%r290, hi}, %fd111; 
	}
	// end inline asm
	shr.u32 	%r133, %r291, 20;
	and.b32  	%r292, %r133, 2047;
	setp.ne.s32 	%p102, %r292, 0;
	@%p102 bra 	$L__BB0_87;

	mov.f64 	%fd678, 0d4350000000000000;
	mul.rn.f64 	%fd677, %fd111, %fd678;
	// begin inline asm
	{ 
	.reg 	.b32 lo; 
	mov.b64 	{lo, %r291}, %fd677; 
	}
	// end inline asm
	// begin inline asm
	{ 
	.reg 	.b32 hi; 
	mov.b64 	{%r290, hi}, %fd677; 
	}
	// end inline asm
	shr.u32 	%r136, %r291, 20;
	and.b32  	%r137, %r136, 2047;
	add.s32 	%r292, %r137, -54;

$L__BB0_87:
	and.b32  	%r140, %r291, -2146435073;
	or.b32  	%r139, %r140, 1072693248;
	// begin inline asm
	mov.b64 	%fd1535, {%r290, %r139};
	// end inline asm
	setp.lt.u32 	%p103, %r139, 1073127583;
	add.s32 	%r293, %r292, -1023;
	@%p103 bra 	$L__BB0_89;

	// begin inline asm
	{ 
	.reg 	.b32 hi; 
	mov.b64 	{%r141, hi}, %fd1535; 
	}
	// end inline asm
	// begin inline asm
	{ 
	.reg 	.b32 lo; 
	mov.b64 	{lo, %r142}, %fd1535; 
	}
	// end inline asm
	add.s32 	%r144, %r142, -1048576;
	// begin inline asm
	mov.b64 	%fd1535, {%r141, %r144};
	// end inline asm
	add.s32 	%r293, %r292, -1022;

$L__BB0_89:
	add.f64 	%fd767, %fd1535, 0d3FF0000000000000;
	mov.f64 	%fd768, 0d3FF0000000000000;
	rcp.rn.f64 	%fd769, %fd767;
	add.f64 	%fd709, %fd1535, 0dBFF0000000000000;
	mul.rn.f64 	%fd770, %fd709, %fd769;
	add.f64 	%fd757, %fd770, %fd770;
	mul.rn.f64 	%fd705, %fd757, %fd757;
	mov.f64 	%fd684, 0d3EB0F5FF7D2CAFE2;
	mov.f64 	%fd686, 0d3ED0F5D241AD3B5A;
	// begin inline asm
	fma.rn.f64 	%fd683, %fd684, %fd705, %fd686;
	// end inline asm
	mov.f64 	%fd690, 0d3EF3B20A75488A3F;
	// begin inline asm
	fma.rn.f64 	%fd687, %fd683, %fd705, %fd690;
	// end inline asm
	mov.f64 	%fd694, 0d3F1745CDE4FAECD5;
	// begin inline asm
	fma.rn.f64 	%fd691, %fd687, %fd705, %fd694;
	// end inline asm
	mov.f64 	%fd698, 0d3F3C71C7258A578B;
	// begin inline asm
	fma.rn.f64 	%fd695, %fd691, %fd705, %fd698;
	// end inline asm
	mov.f64 	%fd702, 0d3F6249249242B910;
	// begin inline asm
	fma.rn.f64 	%fd699, %fd695, %fd705, %fd702;
	// end inline asm
	mov.f64 	%fd706, 0d3F89999999999DFB;
	// begin inline asm
	fma.rn.f64 	%fd703, %fd699, %fd705, %fd706;
	// end inline asm
	mul.rn.f64 	%fd771, %fd703, %fd705;
	sub.f64 	%fd772, %fd709, %fd757;
	mov.f64 	%fd765, 0d4000000000000000;
	mul.rn.f64 	%fd710, %fd765, %fd772;
	neg.f64 	%fd708, %fd757;
	// begin inline asm
	fma.rn.f64 	%fd707, %fd708, %fd709, %fd710;
	// end inline asm
	mul.rn.f64 	%fd753, %fd769, %fd707;
	add.f64 	%fd773, %fd771, 0d3FB5555555555555;
	mov.f64 	%fd774, 0d3FB5555555555555;
	sub.f64 	%fd775, %fd774, %fd773;
	add.f64 	%fd776, %fd771, %fd775;
	add.f64 	%fd777, %fd776, 0d0000000000000000;
	add.f64 	%fd778, %fd777, 0dBC46A4CB00B9E7B0;
	add.f64 	%fd720, %fd773, %fd778;
	sub.f64 	%fd779, %fd773, %fd720;
	add.f64 	%fd724, %fd778, %fd779;
	mul.rn.f64 	%fd780, %fd720, %fd757;
	neg.f64 	%fd714, %fd780;
	// begin inline asm
	fma.rn.f64 	%fd711, %fd720, %fd757, %fd714;
	// end inline asm
	// begin inline asm
	fma.rn.f64 	%fd715, %fd724, %fd753, %fd711;
	// end inline asm
	// begin inline asm
	fma.rn.f64 	%fd719, %fd720, %fd753, %fd715;
	// end inline asm
	// begin inline asm
	fma.rn.f64 	%fd723, %fd724, %fd757, %fd719;
	// end inline asm
	add.f64 	%fd736, %fd780, %fd723;
	sub.f64 	%fd781, %fd780, %fd736;
	add.f64 	%fd740, %fd723, %fd781;
	mul.rn.f64 	%fd782, %fd736, %fd757;
	neg.f64 	%fd730, %fd782;
	// begin inline asm
	fma.rn.f64 	%fd727, %fd736, %fd757, %fd730;
	// end inline asm
	// begin inline asm
	fma.rn.f64 	%fd731, %fd740, %fd753, %fd727;
	// end inline asm
	// begin inline asm
	fma.rn.f64 	%fd735, %fd736, %fd753, %fd731;
	// end inline asm
	// begin inline asm
	fma.rn.f64 	%fd739, %fd740, %fd757, %fd735;
	// end inline asm
	add.f64 	%fd752, %fd782, %fd739;
	sub.f64 	%fd783, %fd782, %fd752;
	add.f64 	%fd756, %fd739, %fd783;
	mul.rn.f64 	%fd784, %fd752, %fd757;
	neg.f64 	%fd746, %fd784;
	// begin inline asm
	fma.rn.f64 	%fd743, %fd752, %fd757, %fd746;
	// end inline asm
	// begin inline asm
	fma.rn.f64 	%fd747, %fd756, %fd753, %fd743;
	// end inline asm
	// begin inline asm
	fma.rn.f64 	%fd751, %fd752, %fd753, %fd747;
	// end inline asm
	// begin inline asm
	fma.rn.f64 	%fd755, %fd756, %fd757, %fd751;
	// end inline asm
	add.f64 	%fd785, %fd784, %fd755;
	sub.f64 	%fd786, %fd784, %fd785;
	add.f64 	%fd787, %fd755, %fd786;
	add.f64 	%fd788, %fd757, %fd785;
	sub.f64 	%fd789, %fd757, %fd788;
	add.f64 	%fd790, %fd785, %fd789;
	add.f64 	%fd791, %fd787, %fd790;
	add.f64 	%fd792, %fd753, %fd791;
	add.f64 	%fd793, %fd788, %fd792;
	sub.f64 	%fd794, %fd788, %fd793;
	add.f64 	%fd795, %fd792, %fd794;
	cvt.rn.f64.s32 	%fd796, %r293;
	mov.f64 	%fd797, 0d3FE62E42FEFA3000;
	mul.rn.f64 	%fd798, %fd796, %fd797;
	mov.f64 	%fd799, 0d3D53DE6AF278ECE6;
	mul.rn.f64 	%fd800, %fd796, %fd799;
	add.f64 	%fd801, %fd798, %fd793;
	sub.f64 	%fd802, %fd798, %fd801;
	add.f64 	%fd803, %fd793, %fd802;
	add.f64 	%fd804, %fd795, %fd803;
	add.f64 	%fd805, %fd800, %fd804;
	add.f64 	%fd760, %fd801, %fd805;
	sub.f64 	%fd806, %fd801, %fd760;
	add.f64 	%fd764, %fd805, %fd806;
	mul.rn.f64 	%fd807, %fd760, %fd765;
	neg.f64 	%fd762, %fd807;
	// begin inline asm
	fma.rn.f64 	%fd759, %fd760, %fd765, %fd762;
	// end inline asm
	// begin inline asm
	fma.rn.f64 	%fd763, %fd764, %fd765, %fd759;
	// end inline asm
	add.f64 	%fd117, %fd807, %fd763;
	sub.f64 	%fd808, %fd807, %fd117;
	add.f64 	%fd118, %fd763, %fd808;
	mov.f64 	%fd809, 0d4338000000000000;
	mov.f64 	%fd810, 0d3FF71547652B82FE;
	fma.rn.f64 	%fd811, %fd117, %fd810, %fd809;
	{
	.reg .b32 %temp; 
	mov.b64 	{%r33, %temp}, %fd811;
	}
	mov.f64 	%fd812, 0dC338000000000000;
	add.rn.f64 	%fd813, %fd811, %fd812;
	mov.f64 	%fd814, 0dBFE62E42FEFA39EF;
	fma.rn.f64 	%fd815, %fd813, %fd814, %fd117;
	mov.f64 	%fd816, 0dBC7ABC9E3B39803F;
	fma.rn.f64 	%fd817, %fd813, %fd816, %fd815;
	mov.f64 	%fd818, 0d3E928AF3FCA213EA;
	mov.f64 	%fd819, 0d3E5ADE1569CE2BDF;
	fma.rn.f64 	%fd820, %fd819, %fd817, %fd818;
	mov.f64 	%fd821, 0d3EC71DEE62401315;
	fma.rn.f64 	%fd822, %fd820, %fd817, %fd821;
	mov.f64 	%fd823, 0d3EFA01997C89EB71;
	fma.rn.f64 	%fd824, %fd822, %fd817, %fd823;
	mov.f64 	%fd825, 0d3F2A01A014761F65;
	fma.rn.f64 	%fd826, %fd824, %fd817, %fd825;
	mov.f64 	%fd827, 0d3F56C16C1852B7AF;
	fma.rn.f64 	%fd828, %fd826, %fd817, %fd827;
	mov.f64 	%fd829, 0d3F81111111122322;
	fma.rn.f64 	%fd830, %fd828, %fd817, %fd829;
	mov.f64 	%fd831, 0d3FA55555555502A1;
	fma.rn.f64 	%fd832, %fd830, %fd817, %fd831;
	mov.f64 	%fd833, 0d3FC5555555555511;
	fma.rn.f64 	%fd834, %fd832, %fd817, %fd833;
	mov.f64 	%fd835, 0d3FE000000000000B;
	fma.rn.f64 	%fd836, %fd834, %fd817, %fd835;
	fma.rn.f64 	%fd837, %fd836, %fd817, %fd768;
	fma.rn.f64 	%fd838, %fd837, %fd817, %fd768;
	{
	.reg .b32 %temp; 
	mov.b64 	{%r34, %temp}, %fd838;
	}
	{
	.reg .b32 %temp; 
	mov.b64 	{%temp, %r35}, %fd838;
	}
	shl.b32 	%r145, %r33, 20;
	add.s32 	%r146, %r35, %r145;
	mov.b64 	%fd1538, {%r34, %r146};
	{
	.reg .b32 %temp; 
	mov.b64 	{%temp, %r147}, %fd117;
	}
	mov.b32 	%f9, %r147;
	abs.f32 	%f1, %f9;
	setp.lt.f32 	%p104, %f1, 0f4086232B;
	@%p104 bra 	$L__BB0_92;

	setp.lt.f64 	%p105, %fd117, 0d0000000000000000;
	add.f64 	%fd839, %fd117, 0d7FF0000000000000;
	selp.f64 	%fd1538, 0d0000000000000000, %fd839, %p105;
	setp.geu.f32 	%p106, %f1, 0f40874800;
	@%p106 bra 	$L__BB0_92;

	mov.f64 	%fd1463, 0d4338000000000000;
	mov.f64 	%fd1462, 0d3FF71547652B82FE;
	fma.rn.f64 	%fd1461, %fd117, %fd1462, %fd1463;
	{
	.reg .b32 %temp; 
	mov.b64 	{%r270, %temp}, %fd1461;
	}
	shr.u32 	%r148, %r270, 31;
	add.s32 	%r149, %r270, %r148;
	shr.s32 	%r150, %r149, 1;
	shl.b32 	%r151, %r150, 20;
	add.s32 	%r152, %r35, %r151;
	mov.b64 	%fd840, {%r34, %r152};
	sub.s32 	%r153, %r270, %r150;
	shl.b32 	%r154, %r153, 20;
	add.s32 	%r155, %r154, 1072693248;
	mov.u32 	%r156, 0;
	mov.b64 	%fd841, {%r156, %r155};
	mul.f64 	%fd1538, %fd840, %fd841;

$L__BB0_92:
	{
	.reg .b32 %temp; 
	mov.b64 	{%temp, %r157}, %fd1538;
	}
	and.b32  	%r158, %r157, 2147483647;
	setp.eq.s32 	%p107, %r158, 2146435072;
	{
	.reg .b32 %temp; 
	mov.b64 	{%r159, %temp}, %fd1538;
	}
	setp.eq.s32 	%p108, %r159, 0;
	and.pred  	%p109, %p108, %p107;
	@%p109 bra 	$L__BB0_94;

	// begin inline asm
	fma.rn.f64 	%fd1538, %fd1538, %fd118, %fd1538;
	// end inline asm

$L__BB0_94:
	abs.f64 	%fd1464, %fd1534;
	setp.geu.f64 	%p354, %fd1464, 0d0000000000000000;
	setp.neu.f64 	%p110, %fd113, 0d3FF0000000000000;
	or.pred  	%p112, %p354, %p110;
	@%p112 bra 	$L__BB0_101;

	mov.b64 	%rd49, %fd1538;
	xor.b64  	%rd50, %rd49, -9223372036854775808;
	mov.b64 	%fd1538, %rd50;
	bra.uni 	$L__BB0_101;

$L__BB0_160:
	setp.geu.f64 	%p192, %fd169, 0d0000000000000000;
	@%p192 bra 	$L__BB0_162;

	mov.f64 	%fd1055, 0d4000000000000000;
	cvt.rzi.f64.f64 	%fd1056, %fd1055;
	setp.neu.f64 	%p193, %fd1056, 0d4000000000000000;
	mov.f64 	%fd1550, 0dFFF8000000000000;
	@%p193 bra 	$L__BB0_178;

$L__BB0_162:
	// begin inline asm
	{ 
	.reg 	.b32 lo; 
	mov.b64 	{lo, %r299}, %fd170; 
	}
	// end inline asm
	// begin inline asm
	{ 
	.reg 	.b32 hi; 
	mov.b64 	{%r298, hi}, %fd170; 
	}
	// end inline asm
	shr.u32 	%r201, %r299, 20;
	and.b32  	%r300, %r201, 2047;
	setp.ne.s32 	%p194, %r300, 0;
	@%p194 bra 	$L__BB0_164;

	mov.f64 	%fd1061, 0d4350000000000000;
	mul.rn.f64 	%fd1060, %fd170, %fd1061;
	// begin inline asm
	{ 
	.reg 	.b32 lo; 
	mov.b64 	{lo, %r299}, %fd1060; 
	}
	// end inline asm
	// begin inline asm
	{ 
	.reg 	.b32 hi; 
	mov.b64 	{%r298, hi}, %fd1060; 
	}
	// end inline asm
	shr.u32 	%r204, %r299, 20;
	and.b32  	%r205, %r204, 2047;
	add.s32 	%r300, %r205, -54;

$L__BB0_164:
	add.s32 	%r301, %r300, -1023;
	and.b32  	%r208, %r299, -2146435073;
	or.b32  	%r207, %r208, 1072693248;
	// begin inline asm
	mov.b64 	%fd1547, {%r298, %r207};
	// end inline asm
	setp.lt.u32 	%p195, %r207, 1073127583;
	@%p195 bra 	$L__BB0_166;

	// begin inline asm
	{ 
	.reg 	.b32 hi; 
	mov.b64 	{%r209, hi}, %fd1547; 
	}
	// end inline asm
	// begin inline asm
	{ 
	.reg 	.b32 lo; 
	mov.b64 	{lo, %r210}, %fd1547; 
	}
	// end inline asm
	add.s32 	%r212, %r210, -1048576;
	// begin inline asm
	mov.b64 	%fd1547, {%r209, %r212};
	// end inline asm
	add.s32 	%r301, %r300, -1022;

$L__BB0_166:
	add.f64 	%fd1150, %fd1547, 0d3FF0000000000000;
	mov.f64 	%fd1151, 0d3FF0000000000000;
	rcp.rn.f64 	%fd1152, %fd1150;
	add.f64 	%fd1092, %fd1547, 0dBFF0000000000000;
	mul.rn.f64 	%fd1153, %fd1092, %fd1152;
	add.f64 	%fd1140, %fd1153, %fd1153;
	mul.rn.f64 	%fd1088, %fd1140, %fd1140;
	mov.f64 	%fd1067, 0d3EB0F5FF7D2CAFE2;
	mov.f64 	%fd1069, 0d3ED0F5D241AD3B5A;
	// begin inline asm
	fma.rn.f64 	%fd1066, %fd1067, %fd1088, %fd1069;
	// end inline asm
	mov.f64 	%fd1073, 0d3EF3B20A75488A3F;
	// begin inline asm
	fma.rn.f64 	%fd1070, %fd1066, %fd1088, %fd1073;
	// end inline asm
	mov.f64 	%fd1077, 0d3F1745CDE4FAECD5;
	// begin inline asm
	fma.rn.f64 	%fd1074, %fd1070, %fd1088, %fd1077;
	// end inline asm
	mov.f64 	%fd1081, 0d3F3C71C7258A578B;
	// begin inline asm
	fma.rn.f64 	%fd1078, %fd1074, %fd1088, %fd1081;
	// end inline asm
	mov.f64 	%fd1085, 0d3F6249249242B910;
	// begin inline asm
	fma.rn.f64 	%fd1082, %fd1078, %fd1088, %fd1085;
	// end inline asm
	mov.f64 	%fd1089, 0d3F89999999999DFB;
	// begin inline asm
	fma.rn.f64 	%fd1086, %fd1082, %fd1088, %fd1089;
	// end inline asm
	mul.rn.f64 	%fd1154, %fd1086, %fd1088;
	sub.f64 	%fd1155, %fd1092, %fd1140;
	mov.f64 	%fd1148, 0d4000000000000000;
	mul.rn.f64 	%fd1093, %fd1148, %fd1155;
	neg.f64 	%fd1091, %fd1140;
	// begin inline asm
	fma.rn.f64 	%fd1090, %fd1091, %fd1092, %fd1093;
	// end inline asm
	mul.rn.f64 	%fd1136, %fd1152, %fd1090;
	add.f64 	%fd1156, %fd1154, 0d3FB5555555555555;
	mov.f64 	%fd1157, 0d3FB5555555555555;
	sub.f64 	%fd1158, %fd1157, %fd1156;
	add.f64 	%fd1159, %fd1154, %fd1158;
	add.f64 	%fd1160, %fd1159, 0d0000000000000000;
	add.f64 	%fd1161, %fd1160, 0dBC46A4CB00B9E7B0;
	add.f64 	%fd1103, %fd1156, %fd1161;
	sub.f64 	%fd1162, %fd1156, %fd1103;
	add.f64 	%fd1107, %fd1161, %fd1162;
	mul.rn.f64 	%fd1163, %fd1103, %fd1140;
	neg.f64 	%fd1097, %fd1163;
	// begin inline asm
	fma.rn.f64 	%fd1094, %fd1103, %fd1140, %fd1097;
	// end inline asm
	// begin inline asm
	fma.rn.f64 	%fd1098, %fd1107, %fd1136, %fd1094;
	// end inline asm
	// begin inline asm
	fma.rn.f64 	%fd1102, %fd1103, %fd1136, %fd1098;
	// end inline asm
	// begin inline asm
	fma.rn.f64 	%fd1106, %fd1107, %fd1140, %fd1102;
	// end inline asm
	add.f64 	%fd1119, %fd1163, %fd1106;
	sub.f64 	%fd1164, %fd1163, %fd1119;
	add.f64 	%fd1123, %fd1106, %fd1164;
	mul.rn.f64 	%fd1165, %fd1119, %fd1140;
	neg.f64 	%fd1113, %fd1165;
	// begin inline asm
	fma.rn.f64 	%fd1110, %fd1119, %fd1140, %fd1113;
	// end inline asm
	// begin inline asm
	fma.rn.f64 	%fd1114, %fd1123, %fd1136, %fd1110;
	// end inline asm
	// begin inline asm
	fma.rn.f64 	%fd1118, %fd1119, %fd1136, %fd1114;
	// end inline asm
	// begin inline asm
	fma.rn.f64 	%fd1122, %fd1123, %fd1140, %fd1118;
	// end inline asm
	add.f64 	%fd1135, %fd1165, %fd1122;
	sub.f64 	%fd1166, %fd1165, %fd1135;
	add.f64 	%fd1139, %fd1122, %fd1166;
	mul.rn.f64 	%fd1167, %fd1135, %fd1140;
	neg.f64 	%fd1129, %fd1167;
	// begin inline asm
	fma.rn.f64 	%fd1126, %fd1135, %fd1140, %fd1129;
	// end inline asm
	// begin inline asm
	fma.rn.f64 	%fd1130, %fd1139, %fd1136, %fd1126;
	// end inline asm
	// begin inline asm
	fma.rn.f64 	%fd1134, %fd1135, %fd1136, %fd1130;
	// end inline asm
	// begin inline asm
	fma.rn.f64 	%fd1138, %fd1139, %fd1140, %fd1134;
	// end inline asm
	add.f64 	%fd1168, %fd1167, %fd1138;
	sub.f64 	%fd1169, %fd1167, %fd1168;
	add.f64 	%fd1170, %fd1138, %fd1169;
	add.f64 	%fd1171, %fd1140, %fd1168;
	sub.f64 	%fd1172, %fd1140, %fd1171;
	add.f64 	%fd1173, %fd1168, %fd1172;
	add.f64 	%fd1174, %fd1170, %fd1173;
	add.f64 	%fd1175, %fd1136, %fd1174;
	add.f64 	%fd1176, %fd1171, %fd1175;
	sub.f64 	%fd1177, %fd1171, %fd1176;
	add.f64 	%fd1178, %fd1175, %fd1177;
	cvt.rn.f64.s32 	%fd1179, %r301;
	mov.f64 	%fd1180, 0d3FE62E42FEFA3000;
	mul.rn.f64 	%fd1181, %fd1179, %fd1180;
	mov.f64 	%fd1182, 0d3D53DE6AF278ECE6;
	mul.rn.f64 	%fd1183, %fd1179, %fd1182;
	add.f64 	%fd1184, %fd1181, %fd1176;
	sub.f64 	%fd1185, %fd1181, %fd1184;
	add.f64 	%fd1186, %fd1176, %fd1185;
	add.f64 	%fd1187, %fd1178, %fd1186;
	add.f64 	%fd1188, %fd1183, %fd1187;
	add.f64 	%fd1143, %fd1184, %fd1188;
	sub.f64 	%fd1189, %fd1184, %fd1143;
	add.f64 	%fd1147, %fd1188, %fd1189;
	mul.rn.f64 	%fd1190, %fd1143, %fd1148;
	neg.f64 	%fd1145, %fd1190;
	// begin inline asm
	fma.rn.f64 	%fd1142, %fd1143, %fd1148, %fd1145;
	// end inline asm
	// begin inline asm
	fma.rn.f64 	%fd1146, %fd1147, %fd1148, %fd1142;
	// end inline asm
	add.f64 	%fd176, %fd1190, %fd1146;
	sub.f64 	%fd1191, %fd1190, %fd176;
	add.f64 	%fd177, %fd1146, %fd1191;
	mov.f64 	%fd1192, 0d4338000000000000;
	mov.f64 	%fd1193, 0d3FF71547652B82FE;
	fma.rn.f64 	%fd1194, %fd176, %fd1193, %fd1192;
	{
	.reg .b32 %temp; 
	mov.b64 	{%r63, %temp}, %fd1194;
	}
	mov.f64 	%fd1195, 0dC338000000000000;
	add.rn.f64 	%fd1196, %fd1194, %fd1195;
	mov.f64 	%fd1197, 0dBFE62E42FEFA39EF;
	fma.rn.f64 	%fd1198, %fd1196, %fd1197, %fd176;
	mov.f64 	%fd1199, 0dBC7ABC9E3B39803F;
	fma.rn.f64 	%fd1200, %fd1196, %fd1199, %fd1198;
	mov.f64 	%fd1201, 0d3E928AF3FCA213EA;
	mov.f64 	%fd1202, 0d3E5ADE1569CE2BDF;
	fma.rn.f64 	%fd1203, %fd1202, %fd1200, %fd1201;
	mov.f64 	%fd1204, 0d3EC71DEE62401315;
	fma.rn.f64 	%fd1205, %fd1203, %fd1200, %fd1204;
	mov.f64 	%fd1206, 0d3EFA01997C89EB71;
	fma.rn.f64 	%fd1207, %fd1205, %fd1200, %fd1206;
	mov.f64 	%fd1208, 0d3F2A01A014761F65;
	fma.rn.f64 	%fd1209, %fd1207, %fd1200, %fd1208;
	mov.f64 	%fd1210, 0d3F56C16C1852B7AF;
	fma.rn.f64 	%fd1211, %fd1209, %fd1200, %fd1210;
	mov.f64 	%fd1212, 0d3F81111111122322;
	fma.rn.f64 	%fd1213, %fd1211, %fd1200, %fd1212;
	mov.f64 	%fd1214, 0d3FA55555555502A1;
	fma.rn.f64 	%fd1215, %fd1213, %fd1200, %fd1214;
	mov.f64 	%fd1216, 0d3FC5555555555511;
	fma.rn.f64 	%fd1217, %fd1215, %fd1200, %fd1216;
	mov.f64 	%fd1218, 0d3FE000000000000B;
	fma.rn.f64 	%fd1219, %fd1217, %fd1200, %fd1218;
	fma.rn.f64 	%fd1220, %fd1219, %fd1200, %fd1151;
	fma.rn.f64 	%fd1221, %fd1220, %fd1200, %fd1151;
	{
	.reg .b32 %temp; 
	mov.b64 	{%r64, %temp}, %fd1221;
	}
	{
	.reg .b32 %temp; 
	mov.b64 	{%temp, %r65}, %fd1221;
	}
	shl.b32 	%r213, %r63, 20;
	add.s32 	%r214, %r65, %r213;
	mov.b64 	%fd1550, {%r64, %r214};
	{
	.reg .b32 %temp; 
	mov.b64 	{%temp, %r215}, %fd176;
	}
	mov.b32 	%f11, %r215;
	abs.f32 	%f3, %f11;
	setp.lt.f32 	%p196, %f3, 0f4086232B;
	@%p196 bra 	$L__BB0_169;

	setp.lt.f64 	%p197, %fd176, 0d0000000000000000;
	add.f64 	%fd1222, %fd176, 0d7FF0000000000000;
	selp.f64 	%fd1550, 0d0000000000000000, %fd1222, %p197;
	setp.geu.f32 	%p198, %f3, 0f40874800;
	@%p198 bra 	$L__BB0_169;

	mov.f64 	%fd1469, 0d4338000000000000;
	mov.f64 	%fd1468, 0d3FF71547652B82FE;
	fma.rn.f64 	%fd1467, %fd176, %fd1468, %fd1469;
	{
	.reg .b32 %temp; 
	mov.b64 	{%r277, %temp}, %fd1467;
	}
	shr.u32 	%r216, %r277, 31;
	add.s32 	%r217, %r277, %r216;
	shr.s32 	%r218, %r217, 1;
	shl.b32 	%r219, %r218, 20;
	add.s32 	%r220, %r65, %r219;
	mov.b64 	%fd1223, {%r64, %r220};
	sub.s32 	%r221, %r277, %r218;
	shl.b32 	%r222, %r221, 20;
	add.s32 	%r223, %r222, 1072693248;
	mov.u32 	%r224, 0;
	mov.b64 	%fd1224, {%r224, %r223};
	mul.f64 	%fd1550, %fd1223, %fd1224;

$L__BB0_169:
	{
	.reg .b32 %temp; 
	mov.b64 	{%temp, %r225}, %fd1550;
	}
	and.b32  	%r226, %r225, 2147483647;
	setp.eq.s32 	%p199, %r226, 2146435072;
	{
	.reg .b32 %temp; 
	mov.b64 	{%r227, %temp}, %fd1550;
	}
	setp.eq.s32 	%p200, %r227, 0;
	and.pred  	%p201, %p200, %p199;
	@%p201 bra 	$L__BB0_171;

	// begin inline asm
	fma.rn.f64 	%fd1550, %fd1550, %fd177, %fd1550;
	// end inline asm

$L__BB0_171:
	abs.f64 	%fd1470, %fd168;
	setp.geu.f64 	%p356, %fd1470, 0d0000000000000000;
	setp.neu.f64 	%p202, %fd172, 0d3FF0000000000000;
	or.pred  	%p204, %p356, %p202;
	@%p204 bra 	$L__BB0_178;

	mov.b64 	%rd56, %fd1550;
	xor.b64  	%rd57, %rd56, -9223372036854775808;
	mov.b64 	%fd1550, %rd57;
	bra.uni 	$L__BB0_178;

$L__BB0_111:
	setp.geu.f64 	%p130, %fd110, 0d0000000000000000;
	@%p130 bra 	$L__BB0_113;

	cvt.rzi.f64.f64 	%fd860, %fd130;
	setp.neu.f64 	%p131, %fd860, %fd130;
	mov.f64 	%fd1543, 0dFFF8000000000000;
	@%p131 bra 	$L__BB0_132;

$L__BB0_113:
	// begin inline asm
	{ 
	.reg 	.b32 lo; 
	mov.b64 	{lo, %r295}, %fd131; 
	}
	// end inline asm
	// begin inline asm
	{ 
	.reg 	.b32 hi; 
	mov.b64 	{%r294, hi}, %fd131; 
	}
	// end inline asm
	shr.u32 	%r166, %r295, 20;
	and.b32  	%r296, %r166, 2047;
	setp.ne.s32 	%p132, %r296, 0;
	@%p132 bra 	$L__BB0_115;

	mov.f64 	%fd865, 0d4350000000000000;
	mul.rn.f64 	%fd864, %fd131, %fd865;
	// begin inline asm
	{ 
	.reg 	.b32 lo; 
	mov.b64 	{lo, %r295}, %fd864; 
	}
	// end inline asm
	// begin inline asm
	{ 
	.reg 	.b32 hi; 
	mov.b64 	{%r294, hi}, %fd864; 
	}
	// end inline asm
	shr.u32 	%r169, %r295, 20;
	and.b32  	%r170, %r169, 2047;
	add.s32 	%r296, %r170, -54;

$L__BB0_115:
	and.b32  	%r173, %r295, -2146435073;
	or.b32  	%r172, %r173, 1072693248;
	// begin inline asm
	mov.b64 	%fd1539, {%r294, %r172};
	// end inline asm
	setp.lt.u32 	%p133, %r172, 1073127583;
	add.s32 	%r297, %r296, -1023;
	@%p133 bra 	$L__BB0_117;

	// begin inline asm
	{ 
	.reg 	.b32 hi; 
	mov.b64 	{%r174, hi}, %fd1539; 
	}
	// end inline asm
	// begin inline asm
	{ 
	.reg 	.b32 lo; 
	mov.b64 	{lo, %r175}, %fd1539; 
	}
	// end inline asm
	add.s32 	%r177, %r175, -1048576;
	// begin inline asm
	mov.b64 	%fd1539, {%r174, %r177};
	// end inline asm
	add.s32 	%r297, %r296, -1022;

$L__BB0_117:
	add.f64 	%fd946, %fd1539, 0d3FF0000000000000;
	rcp.rn.f64 	%fd947, %fd946;
	add.f64 	%fd896, %fd1539, 0dBFF0000000000000;
	mul.rn.f64 	%fd948, %fd896, %fd947;
	add.f64 	%fd944, %fd948, %fd948;
	mul.rn.f64 	%fd892, %fd944, %fd944;
	mov.f64 	%fd871, 0d3EB0F5FF7D2CAFE2;
	mov.f64 	%fd873, 0d3ED0F5D241AD3B5A;
	// begin inline asm
	fma.rn.f64 	%fd870, %fd871, %fd892, %fd873;
	// end inline asm
	mov.f64 	%fd877, 0d3EF3B20A75488A3F;
	// begin inline asm
	fma.rn.f64 	%fd874, %fd870, %fd892, %fd877;
	// end inline asm
	mov.f64 	%fd881, 0d3F1745CDE4FAECD5;
	// begin inline asm
	fma.rn.f64 	%fd878, %fd874, %fd892, %fd881;
	// end inline asm
	mov.f64 	%fd885, 0d3F3C71C7258A578B;
	// begin inline asm
	fma.rn.f64 	%fd882, %fd878, %fd892, %fd885;
	// end inline asm
	mov.f64 	%fd889, 0d3F6249249242B910;
	// begin inline asm
	fma.rn.f64 	%fd886, %fd882, %fd892, %fd889;
	// end inline asm
	mov.f64 	%fd893, 0d3F89999999999DFB;
	// begin inline asm
	fma.rn.f64 	%fd890, %fd886, %fd892, %fd893;
	// end inline asm
	mul.rn.f64 	%fd949, %fd890, %fd892;
	sub.f64 	%fd950, %fd896, %fd944;
	mov.f64 	%fd951, 0d4000000000000000;
	mul.rn.f64 	%fd897, %fd951, %fd950;
	neg.f64 	%fd895, %fd944;
	// begin inline asm
	fma.rn.f64 	%fd894, %fd895, %fd896, %fd897;
	// end inline asm
	mul.rn.f64 	%fd940, %fd947, %fd894;
	add.f64 	%fd952, %fd949, 0d3FB5555555555555;
	mov.f64 	%fd953, 0d3FB5555555555555;
	sub.f64 	%fd954, %fd953, %fd952;
	add.f64 	%fd955, %fd949, %fd954;
	add.f64 	%fd956, %fd955, 0d0000000000000000;
	add.f64 	%fd957, %fd956, 0dBC46A4CB00B9E7B0;
	add.f64 	%fd907, %fd952, %fd957;
	sub.f64 	%fd958, %fd952, %fd907;
	add.f64 	%fd911, %fd957, %fd958;
	mul.rn.f64 	%fd959, %fd907, %fd944;
	neg.f64 	%fd901, %fd959;
	// begin inline asm
	fma.rn.f64 	%fd898, %fd907, %fd944, %fd901;
	// end inline asm
	// begin inline asm
	fma.rn.f64 	%fd902, %fd911, %fd940, %fd898;
	// end inline asm
	// begin inline asm
	fma.rn.f64 	%fd906, %fd907, %fd940, %fd902;
	// end inline asm
	// begin inline asm
	fma.rn.f64 	%fd910, %fd911, %fd944, %fd906;
	// end inline asm
	add.f64 	%fd923, %fd959, %fd910;
	sub.f64 	%fd960, %fd959, %fd923;
	add.f64 	%fd927, %fd910, %fd960;
	mul.rn.f64 	%fd961, %fd923, %fd944;
	neg.f64 	%fd917, %fd961;
	// begin inline asm
	fma.rn.f64 	%fd914, %fd923, %fd944, %fd917;
	// end inline asm
	// begin inline asm
	fma.rn.f64 	%fd918, %fd927, %fd940, %fd914;
	// end inline asm
	// begin inline asm
	fma.rn.f64 	%fd922, %fd923, %fd940, %fd918;
	// end inline asm
	// begin inline asm
	fma.rn.f64 	%fd926, %fd927, %fd944, %fd922;
	// end inline asm
	add.f64 	%fd939, %fd961, %fd926;
	sub.f64 	%fd962, %fd961, %fd939;
	add.f64 	%fd943, %fd926, %fd962;
	mul.rn.f64 	%fd963, %fd939, %fd944;
	neg.f64 	%fd933, %fd963;
	// begin inline asm
	fma.rn.f64 	%fd930, %fd939, %fd944, %fd933;
	// end inline asm
	// begin inline asm
	fma.rn.f64 	%fd934, %fd943, %fd940, %fd930;
	// end inline asm
	// begin inline asm
	fma.rn.f64 	%fd938, %fd939, %fd940, %fd934;
	// end inline asm
	// begin inline asm
	fma.rn.f64 	%fd942, %fd943, %fd944, %fd938;
	// end inline asm
	add.f64 	%fd964, %fd963, %fd942;
	sub.f64 	%fd965, %fd963, %fd964;
	add.f64 	%fd966, %fd942, %fd965;
	add.f64 	%fd967, %fd944, %fd964;
	sub.f64 	%fd968, %fd944, %fd967;
	add.f64 	%fd969, %fd964, %fd968;
	add.f64 	%fd970, %fd966, %fd969;
	add.f64 	%fd971, %fd940, %fd970;
	add.f64 	%fd972, %fd967, %fd971;
	sub.f64 	%fd973, %fd967, %fd972;
	add.f64 	%fd974, %fd971, %fd973;
	cvt.rn.f64.s32 	%fd975, %r297;
	mov.f64 	%fd976, 0d3FE62E42FEFA3000;
	mul.rn.f64 	%fd977, %fd975, %fd976;
	mov.f64 	%fd978, 0d3D53DE6AF278ECE6;
	mul.rn.f64 	%fd979, %fd975, %fd978;
	add.f64 	%fd139, %fd977, %fd972;
	sub.f64 	%fd980, %fd977, %fd139;
	add.f64 	%fd981, %fd972, %fd980;
	add.f64 	%fd982, %fd974, %fd981;
	add.f64 	%fd140, %fd979, %fd982;
	setp.leu.f64 	%p134, %fd132, 0d7F0D2A1BE4048F90;
	@%p134 bra 	$L__BB0_119;

	mov.f64 	%fd983, 0d3F20000000000000;
	mul.rn.f64 	%fd130, %fd130, %fd983;

$L__BB0_119:
	add.f64 	%fd985, %fd139, %fd140;
	mul.rn.f64 	%fd992, %fd985, %fd130;
	neg.f64 	%fd987, %fd992;
	// begin inline asm
	fma.rn.f64 	%fd984, %fd985, %fd130, %fd987;
	// end inline asm
	sub.f64 	%fd993, %fd139, %fd985;
	add.f64 	%fd989, %fd140, %fd993;
	// begin inline asm
	fma.rn.f64 	%fd988, %fd989, %fd130, %fd984;
	// end inline asm
	add.f64 	%fd143, %fd992, %fd988;
	sub.f64 	%fd994, %fd992, %fd143;
	add.f64 	%fd144, %fd988, %fd994;
	mov.f64 	%fd995, 0d4338000000000000;
	mov.f64 	%fd996, 0d3FF71547652B82FE;
	fma.rn.f64 	%fd997, %fd143, %fd996, %fd995;
	{
	.reg .b32 %temp; 
	mov.b64 	{%r48, %temp}, %fd997;
	}
	mov.f64 	%fd998, 0dC338000000000000;
	add.rn.f64 	%fd999, %fd997, %fd998;
	mov.f64 	%fd1000, 0dBFE62E42FEFA39EF;
	fma.rn.f64 	%fd1001, %fd999, %fd1000, %fd143;
	mov.f64 	%fd1002, 0dBC7ABC9E3B39803F;
	fma.rn.f64 	%fd1003, %fd999, %fd1002, %fd1001;
	mov.f64 	%fd1004, 0d3E928AF3FCA213EA;
	mov.f64 	%fd1005, 0d3E5ADE1569CE2BDF;
	fma.rn.f64 	%fd1006, %fd1005, %fd1003, %fd1004;
	mov.f64 	%fd1007, 0d3EC71DEE62401315;
	fma.rn.f64 	%fd1008, %fd1006, %fd1003, %fd1007;
	mov.f64 	%fd1009, 0d3EFA01997C89EB71;
	fma.rn.f64 	%fd1010, %fd1008, %fd1003, %fd1009;
	mov.f64 	%fd1011, 0d3F2A01A014761F65;
	fma.rn.f64 	%fd1012, %fd1010, %fd1003, %fd1011;
	mov.f64 	%fd1013, 0d3F56C16C1852B7AF;
	fma.rn.f64 	%fd1014, %fd1012, %fd1003, %fd1013;
	mov.f64 	%fd1015, 0d3F81111111122322;
	fma.rn.f64 	%fd1016, %fd1014, %fd1003, %fd1015;
	mov.f64 	%fd1017, 0d3FA55555555502A1;
	fma.rn.f64 	%fd1018, %fd1016, %fd1003, %fd1017;
	mov.f64 	%fd1019, 0d3FC5555555555511;
	fma.rn.f64 	%fd1020, %fd1018, %fd1003, %fd1019;
	mov.f64 	%fd1021, 0d3FE000000000000B;
	fma.rn.f64 	%fd1022, %fd1020, %fd1003, %fd1021;
	mov.f64 	%fd1023, 0d3FF0000000000000;
	fma.rn.f64 	%fd1024, %fd1022, %fd1003, %fd1023;
	fma.rn.f64 	%fd1025, %fd1024, %fd1003, %fd1023;
	{
	.reg .b32 %temp; 
	mov.b64 	{%r49, %temp}, %fd1025;
	}
	{
	.reg .b32 %temp; 
	mov.b64 	{%temp, %r50}, %fd1025;
	}
	shl.b32 	%r178, %r48, 20;
	add.s32 	%r179, %r50, %r178;
	mov.b64 	%fd1543, {%r49, %r179};
	{
	.reg .b32 %temp; 
	mov.b64 	{%temp, %r180}, %fd143;
	}
	mov.b32 	%f10, %r180;
	abs.f32 	%f2, %f10;
	setp.lt.f32 	%p135, %f2, 0f4086232B;
	@%p135 bra 	$L__BB0_122;

	setp.lt.f64 	%p136, %fd143, 0d0000000000000000;
	add.f64 	%fd1026, %fd143, 0d7FF0000000000000;
	selp.f64 	%fd1543, 0d0000000000000000, %fd1026, %p136;
	setp.geu.f32 	%p137, %f2, 0f40874800;
	@%p137 bra 	$L__BB0_122;

	shr.u32 	%r181, %r48, 31;
	add.s32 	%r182, %r48, %r181;
	shr.s32 	%r183, %r182, 1;
	shl.b32 	%r184, %r183, 20;
	add.s32 	%r185, %r50, %r184;
	mov.b64 	%fd1027, {%r49, %r185};
	sub.s32 	%r186, %r48, %r183;
	shl.b32 	%r187, %r186, 20;
	add.s32 	%r188, %r187, 1072693248;
	mov.u32 	%r189, 0;
	mov.b64 	%fd1028, {%r189, %r188};
	mul.f64 	%fd1543, %fd1027, %fd1028;

$L__BB0_122:
	{
	.reg .b32 %temp; 
	mov.b64 	{%temp, %r190}, %fd1543;
	}
	and.b32  	%r191, %r190, 2147483647;
	setp.eq.s32 	%p138, %r191, 2146435072;
	{
	.reg .b32 %temp; 
	mov.b64 	{%r192, %temp}, %fd1543;
	}
	setp.eq.s32 	%p139, %r192, 0;
	and.pred  	%p140, %p139, %p138;
	@%p140 bra 	$L__BB0_124;

	// begin inline asm
	fma.rn.f64 	%fd1543, %fd1543, %fd144, %fd1543;
	// end inline asm

$L__BB0_124:
	setp.neu.f64 	%p141, %fd135, 0d3FF0000000000000;
	or.pred  	%p143, %p130, %p141;
	@%p143 bra 	$L__BB0_132;

	mov.b64 	%rd51, %fd1543;
	xor.b64  	%rd52, %rd51, -9223372036854775808;
	mov.b64 	%fd1543, %rd52;
	bra.uni 	$L__BB0_132;

$L__BB0_188:
	abs.f64 	%fd1477, %fd168;
	setp.geu.f64 	%p222, %fd1477, 0d0000000000000000;
	@%p222 bra 	$L__BB0_190;

	cvt.rzi.f64.f64 	%fd1243, %fd189;
	setp.neu.f64 	%p223, %fd1243, %fd189;
	mov.f64 	%fd1555, 0dFFF8000000000000;
	@%p223 bra 	$L__BB0_209;

$L__BB0_190:
	// begin inline asm
	{ 
	.reg 	.b32 lo; 
	mov.b64 	{lo, %r303}, %fd190; 
	}
	// end inline asm
	// begin inline asm
	{ 
	.reg 	.b32 hi; 
	mov.b64 	{%r302, hi}, %fd190; 
	}
	// end inline asm
	shr.u32 	%r234, %r303, 20;
	and.b32  	%r304, %r234, 2047;
	setp.ne.s32 	%p224, %r304, 0;
	@%p224 bra 	$L__BB0_192;

	mov.f64 	%fd1248, 0d4350000000000000;
	mul.rn.f64 	%fd1247, %fd190, %fd1248;
	// begin inline asm
	{ 
	.reg 	.b32 lo; 
	mov.b64 	{lo, %r303}, %fd1247; 
	}
	// end inline asm
	// begin inline asm
	{ 
	.reg 	.b32 hi; 
	mov.b64 	{%r302, hi}, %fd1247; 
	}
	// end inline asm
	shr.u32 	%r237, %r303, 20;
	and.b32  	%r238, %r237, 2047;
	add.s32 	%r304, %r238, -54;

$L__BB0_192:
	add.s32 	%r305, %r304, -1023;
	and.b32  	%r241, %r303, -2146435073;
	or.b32  	%r240, %r241, 1072693248;
	// begin inline asm
	mov.b64 	%fd1551, {%r302, %r240};
	// end inline asm
	setp.lt.u32 	%p225, %r240, 1073127583;
	@%p225 bra 	$L__BB0_194;

	// begin inline asm
	{ 
	.reg 	.b32 hi; 
	mov.b64 	{%r242, hi}, %fd1551; 
	}
	// end inline asm
	// begin inline asm
	{ 
	.reg 	.b32 lo; 
	mov.b64 	{lo, %r243}, %fd1551; 
	}
	// end inline asm
	add.s32 	%r245, %r243, -1048576;
	// begin inline asm
	mov.b64 	%fd1551, {%r242, %r245};
	// end inline asm
	add.s32 	%r305, %r304, -1022;

$L__BB0_194:
	add.f64 	%fd1329, %fd1551, 0d3FF0000000000000;
	rcp.rn.f64 	%fd1330, %fd1329;
	add.f64 	%fd1279, %fd1551, 0dBFF0000000000000;
	mul.rn.f64 	%fd1331, %fd1279, %fd1330;
	add.f64 	%fd1327, %fd1331, %fd1331;
	mul.rn.f64 	%fd1275, %fd1327, %fd1327;
	mov.f64 	%fd1254, 0d3EB0F5FF7D2CAFE2;
	mov.f64 	%fd1256, 0d3ED0F5D241AD3B5A;
	// begin inline asm
	fma.rn.f64 	%fd1253, %fd1254, %fd1275, %fd1256;
	// end inline asm
	mov.f64 	%fd1260, 0d3EF3B20A75488A3F;
	// begin inline asm
	fma.rn.f64 	%fd1257, %fd1253, %fd1275, %fd1260;
	// end inline asm
	mov.f64 	%fd1264, 0d3F1745CDE4FAECD5;
	// begin inline asm
	fma.rn.f64 	%fd1261, %fd1257, %fd1275, %fd1264;
	// end inline asm
	mov.f64 	%fd1268, 0d3F3C71C7258A578B;
	// begin inline asm
	fma.rn.f64 	%fd1265, %fd1261, %fd1275, %fd1268;
	// end inline asm
	mov.f64 	%fd1272, 0d3F6249249242B910;
	// begin inline asm
	fma.rn.f64 	%fd1269, %fd1265, %fd1275, %fd1272;
	// end inline asm
	mov.f64 	%fd1276, 0d3F89999999999DFB;
	// begin inline asm
	fma.rn.f64 	%fd1273, %fd1269, %fd1275, %fd1276;
	// end inline asm
	mul.rn.f64 	%fd1332, %fd1273, %fd1275;
	sub.f64 	%fd1333, %fd1279, %fd1327;
	mov.f64 	%fd1334, 0d4000000000000000;
	mul.rn.f64 	%fd1280, %fd1334, %fd1333;
	neg.f64 	%fd1278, %fd1327;
	// begin inline asm
	fma.rn.f64 	%fd1277, %fd1278, %fd1279, %fd1280;
	// end inline asm
	mul.rn.f64 	%fd1323, %fd1330, %fd1277;
	add.f64 	%fd1335, %fd1332, 0d3FB5555555555555;
	mov.f64 	%fd1336, 0d3FB5555555555555;
	sub.f64 	%fd1337, %fd1336, %fd1335;
	add.f64 	%fd1338, %fd1332, %fd1337;
	add.f64 	%fd1339, %fd1338, 0d0000000000000000;
	add.f64 	%fd1340, %fd1339, 0dBC46A4CB00B9E7B0;
	add.f64 	%fd1290, %fd1335, %fd1340;
	sub.f64 	%fd1341, %fd1335, %fd1290;
	add.f64 	%fd1294, %fd1340, %fd1341;
	mul.rn.f64 	%fd1342, %fd1290, %fd1327;
	neg.f64 	%fd1284, %fd1342;
	// begin inline asm
	fma.rn.f64 	%fd1281, %fd1290, %fd1327, %fd1284;
	// end inline asm
	// begin inline asm
	fma.rn.f64 	%fd1285, %fd1294, %fd1323, %fd1281;
	// end inline asm
	// begin inline asm
	fma.rn.f64 	%fd1289, %fd1290, %fd1323, %fd1285;
	// end inline asm
	// begin inline asm
	fma.rn.f64 	%fd1293, %fd1294, %fd1327, %fd1289;
	// end inline asm
	add.f64 	%fd1306, %fd1342, %fd1293;
	sub.f64 	%fd1343, %fd1342, %fd1306;
	add.f64 	%fd1310, %fd1293, %fd1343;
	mul.rn.f64 	%fd1344, %fd1306, %fd1327;
	neg.f64 	%fd1300, %fd1344;
	// begin inline asm
	fma.rn.f64 	%fd1297, %fd1306, %fd1327, %fd1300;
	// end inline asm
	// begin inline asm
	fma.rn.f64 	%fd1301, %fd1310, %fd1323, %fd1297;
	// end inline asm
	// begin inline asm
	fma.rn.f64 	%fd1305, %fd1306, %fd1323, %fd1301;
	// end inline asm
	// begin inline asm
	fma.rn.f64 	%fd1309, %fd1310, %fd1327, %fd1305;
	// end inline asm
	add.f64 	%fd1322, %fd1344, %fd1309;
	sub.f64 	%fd1345, %fd1344, %fd1322;
	add.f64 	%fd1326, %fd1309, %fd1345;
	mul.rn.f64 	%fd1346, %fd1322, %fd1327;
	neg.f64 	%fd1316, %fd1346;
	// begin inline asm
	fma.rn.f64 	%fd1313, %fd1322, %fd1327, %fd1316;
	// end inline asm
	// begin inline asm
	fma.rn.f64 	%fd1317, %fd1326, %fd1323, %fd1313;
	// end inline asm
	// begin inline asm
	fma.rn.f64 	%fd1321, %fd1322, %fd1323, %fd1317;
	// end inline asm
	// begin inline asm
	fma.rn.f64 	%fd1325, %fd1326, %fd1327, %fd1321;
	// end inline asm
	add.f64 	%fd1347, %fd1346, %fd1325;
	sub.f64 	%fd1348, %fd1346, %fd1347;
	add.f64 	%fd1349, %fd1325, %fd1348;
	add.f64 	%fd1350, %fd1327, %fd1347;
	sub.f64 	%fd1351, %fd1327, %fd1350;
	add.f64 	%fd1352, %fd1347, %fd1351;
	add.f64 	%fd1353, %fd1349, %fd1352;
	add.f64 	%fd1354, %fd1323, %fd1353;
	add.f64 	%fd1355, %fd1350, %fd1354;
	sub.f64 	%fd1356, %fd1350, %fd1355;
	add.f64 	%fd1357, %fd1354, %fd1356;
	cvt.rn.f64.s32 	%fd1358, %r305;
	mov.f64 	%fd1359, 0d3FE62E42FEFA3000;
	mul.rn.f64 	%fd1360, %fd1358, %fd1359;
	mov.f64 	%fd1361, 0d3D53DE6AF278ECE6;
	mul.rn.f64 	%fd1362, %fd1358, %fd1361;
	add.f64 	%fd198, %fd1360, %fd1355;
	sub.f64 	%fd1363, %fd1360, %fd198;
	add.f64 	%fd1364, %fd1355, %fd1363;
	add.f64 	%fd1365, %fd1357, %fd1364;
	add.f64 	%fd199, %fd1362, %fd1365;
	setp.leu.f64 	%p226, %fd191, 0d7F0D2A1BE4048F90;
	@%p226 bra 	$L__BB0_196;

	mov.f64 	%fd1366, 0d3F20000000000000;
	mul.rn.f64 	%fd189, %fd189, %fd1366;

$L__BB0_196:
	add.f64 	%fd1368, %fd198, %fd199;
	mul.rn.f64 	%fd1375, %fd1368, %fd189;
	neg.f64 	%fd1370, %fd1375;
	// begin inline asm
	fma.rn.f64 	%fd1367, %fd1368, %fd189, %fd1370;
	// end inline asm
	sub.f64 	%fd1376, %fd198, %fd1368;
	add.f64 	%fd1372, %fd199, %fd1376;
	// begin inline asm
	fma.rn.f64 	%fd1371, %fd1372, %fd189, %fd1367;
	// end inline asm
	add.f64 	%fd202, %fd1375, %fd1371;
	sub.f64 	%fd1377, %fd1375, %fd202;
	add.f64 	%fd203, %fd1371, %fd1377;
	mov.f64 	%fd1378, 0d4338000000000000;
	mov.f64 	%fd1379, 0d3FF71547652B82FE;
	fma.rn.f64 	%fd1380, %fd202, %fd1379, %fd1378;
	{
	.reg .b32 %temp; 
	mov.b64 	{%r78, %temp}, %fd1380;
	}
	mov.f64 	%fd1381, 0dC338000000000000;
	add.rn.f64 	%fd1382, %fd1380, %fd1381;
	mov.f64 	%fd1383, 0dBFE62E42FEFA39EF;
	fma.rn.f64 	%fd1384, %fd1382, %fd1383, %fd202;
	mov.f64 	%fd1385, 0dBC7ABC9E3B39803F;
	fma.rn.f64 	%fd1386, %fd1382, %fd1385, %fd1384;
	mov.f64 	%fd1387, 0d3E928AF3FCA213EA;
	mov.f64 	%fd1388, 0d3E5ADE1569CE2BDF;
	fma.rn.f64 	%fd1389, %fd1388, %fd1386, %fd1387;
	mov.f64 	%fd1390, 0d3EC71DEE62401315;
	fma.rn.f64 	%fd1391, %fd1389, %fd1386, %fd1390;
	mov.f64 	%fd1392, 0d3EFA01997C89EB71;
	fma.rn.f64 	%fd1393, %fd1391, %fd1386, %fd1392;
	mov.f64 	%fd1394, 0d3F2A01A014761F65;
	fma.rn.f64 	%fd1395, %fd1393, %fd1386, %fd1394;
	mov.f64 	%fd1396, 0d3F56C16C1852B7AF;
	fma.rn.f64 	%fd1397, %fd1395, %fd1386, %fd1396;
	mov.f64 	%fd1398, 0d3F81111111122322;
	fma.rn.f64 	%fd1399, %fd1397, %fd1386, %fd1398;
	mov.f64 	%fd1400, 0d3FA55555555502A1;
	fma.rn.f64 	%fd1401, %fd1399, %fd1386, %fd1400;
	mov.f64 	%fd1402, 0d3FC5555555555511;
	fma.rn.f64 	%fd1403, %fd1401, %fd1386, %fd1402;
	mov.f64 	%fd1404, 0d3FE000000000000B;
	fma.rn.f64 	%fd1405, %fd1403, %fd1386, %fd1404;
	mov.f64 	%fd1406, 0d3FF0000000000000;
	fma.rn.f64 	%fd1407, %fd1405, %fd1386, %fd1406;
	fma.rn.f64 	%fd1408, %fd1407, %fd1386, %fd1406;
	{
	.reg .b32 %temp; 
	mov.b64 	{%r79, %temp}, %fd1408;
	}
	{
	.reg .b32 %temp; 
	mov.b64 	{%temp, %r80}, %fd1408;
	}
	shl.b32 	%r246, %r78, 20;
	add.s32 	%r247, %r80, %r246;
	mov.b64 	%fd1555, {%r79, %r247};
	{
	.reg .b32 %temp; 
	mov.b64 	{%temp, %r248}, %fd202;
	}
	mov.b32 	%f12, %r248;
	abs.f32 	%f4, %f12;
	setp.lt.f32 	%p227, %f4, 0f4086232B;
	@%p227 bra 	$L__BB0_199;

	setp.lt.f64 	%p228, %fd202, 0d0000000000000000;
	add.f64 	%fd1409, %fd202, 0d7FF0000000000000;
	selp.f64 	%fd1555, 0d0000000000000000, %fd1409, %p228;
	setp.geu.f32 	%p229, %f4, 0f40874800;
	@%p229 bra 	$L__BB0_199;

	shr.u32 	%r249, %r78, 31;
	add.s32 	%r250, %r78, %r249;
	shr.s32 	%r251, %r250, 1;
	shl.b32 	%r252, %r251, 20;
	add.s32 	%r253, %r80, %r252;
	mov.b64 	%fd1410, {%r79, %r253};
	sub.s32 	%r254, %r78, %r251;
	shl.b32 	%r255, %r254, 20;
	add.s32 	%r256, %r255, 1072693248;
	mov.u32 	%r257, 0;
	mov.b64 	%fd1411, {%r257, %r256};
	mul.f64 	%fd1555, %fd1410, %fd1411;

$L__BB0_199:
	{
	.reg .b32 %temp; 
	mov.b64 	{%temp, %r258}, %fd1555;
	}
	and.b32  	%r259, %r258, 2147483647;
	setp.eq.s32 	%p230, %r259, 2146435072;
	{
	.reg .b32 %temp; 
	mov.b64 	{%r260, %temp}, %fd1555;
	}
	setp.eq.s32 	%p231, %r260, 0;
	and.pred  	%p232, %p231, %p230;
	@%p232 bra 	$L__BB0_201;

	// begin inline asm
	fma.rn.f64 	%fd1555, %fd1555, %fd203, %fd1555;
	// end inline asm

$L__BB0_201:
	setp.neu.f64 	%p233, %fd194, 0d3FF0000000000000;
	or.pred  	%p235, %p222, %p233;
	@%p235 bra 	$L__BB0_209;

	mov.b64 	%rd58, %fd1555;
	xor.b64  	%rd59, %rd58, -9223372036854775808;
	mov.b64 	%fd1555, %rd59;
	bra.uni 	$L__BB0_209;

}

  